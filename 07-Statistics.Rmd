# Основы статистики {#stat_analysis}

```{r setup-stats, echo = FALSE, purl = FALSE, cache = FALSE, include=FALSE}
knitr::opts_knit$set(global.par = TRUE)
knitr::opts_chunk$set(warning=FALSE, message = FALSE, collapse=TRUE)
```

## Предварительные требования {#stat_analysis_prerequisites}

Для работы по теме текущей лекции вам понадобятся пакеты из __tidyverse__. Помимо этого, мы будем работать с данными через интерфейс _Google Sheets_ напрямую с использованием пакетов [__googledrive__](https://googledrive.tidyverse.org/) и [__googlesheets4__](https://googlesheets4.tidyverse.org/). Также в лекции используется пакет __ggrepel__, позволяющий устранять конфликты подписей на графиках __ggplot__:
```{r}
library(tidyverse)
library(googledrive)
library(googlesheets4)
library(ggrepel)
```

> __Внимание:__ пакет __googlesheets4__ в настоящий момент не доступен на CRAN. Для его установки вам необходимо сначала стандартным путём установить пакет [__devtools__](https://devtools.r-lib.org/), а затем вызвать в консоли команду `devtools::install_github("tidyverse/googlesheets4")`. Эта команда устанавливает пакет из репозитория GitHub.

## Введение {#stat_analysis_intro}

__Математическая статистика__ --- раздел математики, посвящённый математическим методам систематизации, обработки и использования статистических данных для научных и практических выводов. Под статистическими данными обычно понимают числовую информацию, извлекаемую из результатов выборочных обследований, результаты серии неточных измерений и вообще любую систему количественных данных [@bigenc:matstat].

Статистический метод представляет собой важнейший инструмент исследования, применяющийся во всех без исключения областях науки и технологий. Математическая статистика тесно связана с теорией вероятностей -- разделом математики, изучающим математические модели случайных явлений. В силу огромного разнообразия статистических методов и специфики их применения в разных приложения, в одной лекции нет возможности (и смысла) представить их в одной лекции. 

В связи с этим в настоящем разделе представляются основные инструменты статистики, такие как: простейшие приемы статистического описания (описательные статистики), проверка статистических гипотез, оценка плотности распределения, корреляция и регрессия. Помимо этого, в настоящем разделе большое внимание уделено построению специализированных графиков, отражающих особенности распределения величины: гистограмм, диаграмм размаха, линий регрессии и локальной регрессии, кривых и поверхностей плотности распределения.

### Источники данных {#stat_analysis_intro_sources}

#### База данных Gapminder {#stat_analysis_intro_gapminder}

В данной лекции мы будем работать с базой данных [__Gapminder__](https://www.gapminder.org/), которая содержит уникальный набор показателей по странам мира, агрегированный из различных источников (многие показатели имеют ряды на несколько столетий!):

![База данных Gapminder](images/gapminder.png)

__Gapminder__ отлично подходит для знакомства со основами статистического  анализа в R, поскольку эта база данных содержит показатели с разным видом распределения, которые сгруппированы по макрорегионам и континентам, и, разумеется, имеют между собой ряд взаимосвязей, совместное поведение которых можно изучать посредством корреляционного и регрессионного анализа.

Данные __Gapminder__ можно загружать в текстовом формате и формате _Microsoft Excel_, но куда интереснее делать это непосредственно онлайн через программный интерфейс Google Sheets. Для этого нам потребуется знать ключ каждой таблицы, которую мы загружаем. Ключ можно определить, нажав "лупу" для просмотра данных и скопировав его из адресной строки (см. выделение):

![Ключ таблицы Google Sheets из базы данных Gapminder](images/gapminder_key.png)

#### Пакет googlesheets {#stat_analysis_intro_gapminder_google}

Доступ к облачным таблицам — удобный способ работы с табличными данными, который позволяет избавиться от манипуляций с локальными файлами. Свои данные вы тоже можете хранить в таблицах Google. Если таблицы регулярно обновляются держателем данных, загрузка их из облачного хранилища будет гарантировать вам актуальность анализируемой информации. Ограниченем такого режима работы является то, что для доступа к данным вам нужен Интернет.

Пакет [__googlesheets4__](https://googlesheets4.tidyverse.org/) разработан профессором статистики [Дженнифер Брайан](https://github.com/jennybc) для обеспечения онлайн-доступа к таблицам Google. С кратким руководством по использованию пакета вы можете ознакомиться [тут](https://googlesheets4.tidyverse.org/articles/articles/drive-and-sheets.html). Данный пакет использует версию _4.x Google Sheets API_ (отсюда цифра 4 в навании) и рекомендуется к использованию вместо устаревшего пакета [__googlesheets__](https://cran.r-project.org/web/packages/googlesheets/). 

Пакет __googlesheets4__ работает в связке с пакетом __googledrive__, обеспечивающим общие методы доступа к Google Drive. Стандартная последовательность действий чтобы получить в текущей сессии таблицу с _Google Drive_, выглядит так:

1. Подключить библиотеки __googledrive__ и __googlesheets4__.
2. Выполнить авторизацию пользователя путем вызова `drive_auth()`. При вызове этой функции откроется браузер на странице аутентификации Google. Вам необходимо будет войти в систему под своей учетной записью Google.
3. Загрузить таблицу с помощью функции `drive_get()`, передав ей в качестве аргумента название таблицы (в этом случае она будет искаться на _вашем_ диске Google) или идентификатор таблицы (этим способом можно выгрузить в среду R таблицу из любого открытого хранилища Google Drive). Если передается идентификатор таблицы, его необходимо обернуть в класс `drive_id` путем вызова функции `as_id()`.

Рассмотрим как это работает на примере загрузки данных из хранилища _Gapminder_.

## Одна переменная {#stat_analysis_one}

### Оценка распределения {#stat_analysis_one_distr}

Для оценки распределения случайной величины можно использовать графические и статистические способы. Выявление типа распределения важно, поскольку статистические методы не универсальны, и во многих случаях предполагают, что изучаемая переменная подчиняется определенному закону распределения (как правило, нормальному).

В качестве примера возьмем данные по ВВП на душу населения, они содержатся в таблице gapminder с ниже приведенным кодом
```{r}
('1cxtzRRN6ldjSGoDzFHkB8vqPavq1iOTMElGewQnmHgg' %>% ### ВВП на душу населения
  as_id() %>% # преобразуем идентификатор в класс drive_id чтобы отличать его от пути
  drive_get() %>% 
  read_sheet() -> gdpdf) # выгружаем данные по ВВП на душу населения и сохраняем в переменную incdf
```

Для дальнейшей работы целесообразно привести данные к аккуратному виду, избавившись от множества столбцов с годом измерения. Сразу получим данные за 2015 год для анализа:
```{r}
(gdpdf_tidy = gdpdf %>% 
   pivot_longer(cols = `1764`:`2018`, names_to = 'year', values_to = 'gdp') %>% 
   rename(Country = 1))

gdpdf15 = filter(gdpdf_tidy, year == 2015)
```

Для визуальной проверки вида распределения можно использовать геометрию `geom_histogram()`:
```{r}
ggplot(gdpdf15, aes(x = gdp)) + 
  geom_histogram()
```

Изменить ширину кармана можно, используя параметр `binwidth`:
```{r}
ggplot(gdpdf15, aes(x = gdp)) + 
  geom_histogram(binwidth = 5000, color = 'black', fill = 'steelblue', size = 0.2)
```

Аналогично рассмотрим показатель ожидаемой продолжительности жизни:
```{r}
('1H3nzTwbn8z4lJ5gJ_WfDgCeGEXK3PVGcNjQ_U5og8eo' %>% # продолжительность жизни
  as_id() %>% # преобразуем идентификатор в класс drive_id чтобы отличать его от пути
  drive_get() %>% 
  read_sheet()  -> lifedf) # выгружаем данные по ВВП на душу населения и сохраняем в переменную lifedf
```

Преобразуем в аккуратный вид и строим гистограмму распределения:
```{r}
lifedf_tidy = lifedf %>% 
  pivot_longer(cols = `1800`:`2016`, names_to = 'year', values_to = 'lifexp') %>% 
  rename(Country = 1)

lifedf15 = filter(lifedf_tidy, year == 2015)

ggplot(lifedf15, aes(x = lifexp)) + 
  geom_histogram(binwidth = 2, color = 'black', fill = 'olivedrab', size = 0.2)
```

Для графической оценки распределения удобно использовать не только гистограмму, но также метод __ядерного сглаживания__ (_kernel density_), который позволяет строить аппроксимацию функции плотности вероятности. Условно говоря, ядро является функцией, которая позволяет распространить потенциал каждого элемента выборки на его ближайшую окрестность. Чем больше элементов выборки сконцентрировано вблизи данной точки, тем сильнее будет их совокупно наведенный потенциал в данной точке, и тем, соответственно, выше оценка плотности распределения, которая получается суперпозицией этих потенциалов. Математически операция ядерной оценки плотности в точке $x$ определяется как:
$$
\hat f_h (x) = \frac{1}{nh}\sum_{i=1}^{n}K\Big(\frac{x - x_i}{h}\Big)
$$
где $K$ — ядерная функция, $h > 0$ — сглаживающий параметр, $x_i$ — элементы выборки, $n$ — размер выборки. Ядерная функция должна удовлетворять двум критериям: $K(x) \geq 0$, $\int_{-\infty}^{+\infty} K(x) dx = 1$. Отсюда ясно, что любая модель функции плотности распределения может быть использована в качестве ядра: равномерное, нормальное и т.д. Как правило, ядерная функция носит бесконечно убывающий характер: чем дальше мы находимся от точки, тем меньше ее вклад в плотность распределения. 

В __ggplot__ за аппроксимацию плотности распределения методом ядерного сглаживания отвечает геометрия `geom_density()`:
```{r}
ggplot(gdpdf15, aes(x = gdp)) + 
  geom_density(color = 'black', fill = 'steelblue', alpha = 0.5)

ggplot(lifedf15, aes(x = lifexp)) + 
  geom_density(color = 'black', fill = 'olivedrab', alpha = 0.5)
```

Вы можете комбинировать гистограммы и оценку плотности распределения, но для этого гистограмма по оси _Y_ должна отражать не фактическое количество элементов в каждом классе, а _долю_ или плотность вероятности (`y = stat(density)`):
```{r}
ggplot(gdpdf15, aes(x = gdp)) + 
  geom_histogram(aes(y = stat(density)), fill = 'grey', color = 'black', size = 0.1) +
  geom_density(color = 'black', fill = 'steelblue', alpha = 0.5)

ggplot(lifedf15, aes(x = lifexp)) + 
  geom_histogram(aes(y = stat(density)), fill = 'grey', color = 'black', size = 0.1) +
  geom_density(color = 'black', fill = 'olivedrab', alpha = 0.5)
```

При построении гистограмм и оценке плотности распределения мы допустили ошибку: приняли, что все измерения являются равнозначными. Однако в данном случае это не так. Население Люксембурга и Пакистана отличается на два порядка --- это означает, что Пакистан должен иметь соответственно больший вес при построении гистограммы. Для учета этой характеристики подгрузим из __Gapminder__ данные по численности населения и присоединим их к нашим таблицам по ВВП и продолжительности жизни:
```{r}
'1IbDM8z5XicMIXgr93FPwjgwoTTKMuyLfzU6cQrGZzH8' %>% # численность населения
  as_id() %>% # преобразуем идентификатор в класс drive_id чтобы отличать его от пути
  drive_get() %>% 
  read_sheet() %>% # первый лист
  pivot_longer(cols = `1800`:`2015`, names_to = 'year', values_to = 'pop') %>% 
  rename(Country = 1) -> popdf_tidy

(tab = gdpdf_tidy %>% inner_join(lifedf_tidy) %>% inner_join(popdf_tidy))
```

Теперь мы можем произвести взвешенную оценку плотности распределения:
```{r}
tab15 = filter(tab, year == 2015) %>% drop_na() # все веса должны быть непустыми!

ggplot(tab15, aes(x = gdp, y = stat(density), weight = pop/sum(pop))) + 
  geom_histogram(binwidth = 5000, fill = 'grey', color = 'black', size = 0.1) +
  geom_density(color = 'black', fill = 'steelblue', alpha = 0.5)

ggplot(tab15, aes(x = lifexp, y = stat(density), weight = pop/sum(pop))) + 
  geom_histogram(binwidth = 2.5, fill = 'grey', color = 'black', size = 0.1) +
  geom_density(color = 'black', fill = 'olivedrab', alpha = 0.5)
```

Графики плотности распределения удобны тем, что их, в отличие от гистограмм, удобно комбинировать на одном изображении, используя цвет для разделения по еще одной переменной.  Например, мы можем оценить, как изменились мировые диспропорции в продолжительности жизни и доходов населения за последние 50 лет (обратите внимание на параметр `fill = year` в эстетике:
```{r}
tab85 = tab %>% filter(year %in%  c(1965, 2015)) %>% drop_na()

ggplot(tab85, aes(x = gdp, fill = year, weight = pop/sum(pop))) + 
  geom_density(alpha = 0.5)

ggplot(tab85, aes(x = lifexp, fill = year, weight = pop/sum(pop))) + 
  geom_density(alpha = 0.5)
```

### Описательные статистики {#stat_analysis_one_summary}

Описательные статистики --- это числовые характеристики, описывающие особенности статистического распределения изучаемой величины. К таким характеристикам относят выборочное среднее, медиану, минимум, максимум и ряд других величин. Можно вычислять эти характеристики для всей выборки, но для включения географического контекста мы стратифицируем ее по макрорегионам, которые используются в базе данных __Gapminder__. Подгрузим эту информацию (данные скачиваются [отсюда](https://www.gapminder.org/data/geo/)):
```{r, fig.height=10}
library(readxl)
countries = read_excel('data/gapminder.xlsx', 2) %>%
  select(Country = name, Region = eight_regions) %>%
  mutate(Country = factor(Country, levels = Country[order(.$Region)]))

# '1IbDM8z5XicMIXgr93FPwjgwoTTKMuyLfzU6cQrGZzH8' %>% # численность населения
#   as_id() %>% # преобразуем идентификатор в класс drive_id чтобы отличать его от пути
#   drive_get() %>% 
#   read_sheet(sheet = 2) -> countries

ggplot(countries, aes(x = Country, y = 1, fill = Region)) +
  geom_col() +
  geom_text(aes(y = 0.5, label = Country), size = 3) +
  facet_wrap(~Region, scales = "free", ncol = 4) +
  theme_bw()+
  theme(panel.grid = element_blank(),
        axis.title.y = element_blank(),
        axis.text.y = element_blank(),
        axis.ticks.y = element_blank(),
        axis.title.x = element_blank(),
        axis.text.x = element_blank(),
        axis.ticks.x = element_blank()) +
  guides(fill=FALSE) +
  coord_flip()
```

Присоединим эти данные к исходной таблице:
```{r}
(tabreg = tab %>% 
  left_join(countries) %>% 
  filter(year == 2015) %>% 
  drop_na())
```

Мы уже знакомы с функциями `min()`, `max()`, `median()`, `mean()`, `sd()`, которые дают значения соответствующих описательных статистик для векторов данных. Как представить их все одновременно? Для визуализации отличий в статистических параметрах исследуемой выборки удобно использовать тип графика, который называется [__boxplot__](https://ru.wikipedia.org/wiki/%D0%AF%D1%89%D0%B8%D0%BA_%D1%81_%D1%83%D1%81%D0%B0%D0%BC%D0%B8) (а по русски — диаграмма размаха, улей, или ящик с усами). В __ggplot__ за него отвечает геометрия `geom_boxplot()`:

```{r}
ggplot(tabreg, aes(x = Region, y = gdp)) +
  geom_boxplot() + coord_flip()

ggplot(tabreg, aes(x = Region, y = lifexp)) +
  geom_boxplot() + coord_flip()
```

Данные графики наглядно показывают, что регионы отличаются по ряду статистических параметров исследуемой переменной: среднему значению, размаху вариации (разбросу значений), среднеквадратическому отклонению  Эти статистики можно получить и в табличном виде:
```{r}
(tabreg %>% 
  group_by(Region) %>% 
  summarise(gdp_mean = mean(gdp),
            gdp_sd = sd(gdp),
            lifexp_mean = mean(lifexp),
            lifexp_sd = sd(lifexp)))
```

### Статистические тесты {#stat_analysis_one_tests}

Прежде чем манипулировать вычисленными статистиками (говорить, что в Западной Европе ВВП на душу населения в 10 раз выше, чем в Южной Африке), необходимо убедиться, что их отличия являются статистически значимыми. На статистическую значимость влияет не только абсолютная разность средних, но также характер распределения и объем выборки --- выборки малого объема не могут дать высокой статистической значимости.

Для сравнения средних значений и дисперсий двух статистических выборок обычно используют [__тест Стьюдента__](https://ru.wikipedia.org/wiki/T-%D0%9A%D1%80%D0%B8%D1%82%D0%B5%D1%80%D0%B8%D0%B9_%D0%A1%D1%82%D1%8C%D1%8E%D0%B4%D0%B5%D0%BD%D1%82%D0%B0) и [__тест Фишера__](https://ru.wikipedia.org/wiki/F-%D1%82%D0%B5%D1%81%D1%82) соответственно. 

Проведем тесты для сравнения средних по Европе и Южной Африке используя функцию `t.test()` (на самом деле это [тест Уэлча](https://en.wikipedia.org/wiki/Welch%27s_t-test), являющийся модификацией теста Стьюдента):
```{r}
t.test(tabreg %>% filter(Region == 'africa_sub_saharan') %>% pull(gdp),
       tabreg %>% filter(Region == 'europe_west') %>% pull(gdp))

t.test(tabreg %>% filter(Region == 'africa_sub_saharan') %>% pull(lifexp),
       tabreg %>% filter(Region == 'europe_west') %>% pull(lifexp))
```

[__p-значения__](https://ru.wikipedia.org/wiki/P-%D0%B7%D0%BD%D0%B0%D1%87%D0%B5%D0%BD%D0%B8%D0%B5) для данных тестов очень малы, что позволяет нам принять (не отвергать) гипотезу о неравенстве средних для Западной Европы и Южной Африки.

Проверим, так ли значимы отличия в средних для Северной и Южной Америки:
```{r}
t.test(tabreg %>% filter(Region == 'america_north') %>% pull(gdp),
       tabreg %>% filter(Region == 'america_south') %>% pull(gdp))

t.test(tabreg %>% filter(Region == 'america_north') %>% pull(lifexp),
       tabreg %>% filter(Region == 'america_south') %>% pull(lifexp))
```

В данном случае, несмотря на то, что вычисленные значения средних отличаются, тест показывает очень высокие __p-значения__ (0.25 и 0.84 соответственно), что не позволяет нам говорить о том, что эти отличия статистически значимы. Соответственно, делать на их основе какие-либо научные выводы нельзя.

Аналогичным образом можно проверить статистическую значимость отличий в дисперсии (вариации значений) для разных регионов. Для этого используем функцию `var.test()` применительно к регионам Западной и Восточной Европы:
```{r}
var.test(tabreg %>% filter(Region == 'europe_east') %>% pull(gdp),
       tabreg %>% filter(Region == 'europe_west') %>% pull(gdp))

var.test(tabreg %>% filter(Region == 'europe_east') %>% pull(lifexp),
       tabreg %>% filter(Region == 'europe_west') %>% pull(lifexp))
```
Данный тест показывает, что отличия в вариации значений ВВП на душу населения для Западной и Восточной Европы носят пограничный характер (p = 0.04), и принимать их можно только если стоит относительно высокое пороговое значение p = 0.05. В то же время, вариация продолжительности жизни для Западной Европы существенной меньше, чем для Восточной и при данной выборке это отличие обладает высокой статистической значимостью (p = 0.0007). Соответственно, его можно принимать с уверенностью.

## Две переменных {#stat_analysis_two}

Достаточно часто в задачах анализа данных возникает необходимость совместного изучения нескольких переменных. Данный раздел посвящен анализу двух переменных.

### Оценка распределения {#stat_analysis_two_distr}

#### Диаграмма рассеяния {#stat_analysis_two_distr_scatter}

Первичный анализ производится путем оценки совместного распределения переменных на плоскости (для двух переменных) путем построения диаграммы рассеяния. С этим графиком мы уже хорошо знакомы:
```{r}
ggplot(tabreg, aes(gdp, lifexp)) +
  geom_point()
```

Очевидно, что в данном случае мы имеем с нелинейной зависимостью. Чтобы упростить задачу по дальнейшему анализу, можно попробовать перейти к логарифмической шкале по оси _X_:
```{r}
options(scipen = 999)
ggplot(tabreg, aes(gdp, lifexp)) +
  geom_point() +
  scale_x_log10()
```

На диаграмме рассеяния важно показать не только местоположение точек, но также их весовую значимость, которая в данном случае определяется численностью населения в стране. Введем соответствующую графическую переменную — размер точки:
```{r}
ggplot(tabreg, aes(gdp, lifexp, size = pop)) +
  geom_point(alpha = 0.5) +
  scale_x_log10()
```

Еще сильнее повысить информативность диаграммы рассеяния можно, используя цвет точек для обозначения региона принадлежности. Это позволит понять связь между введенной нами _географической стратификацией_ и распределением элементов выборки на диаграмме рассеяния:
```{r}
ggplot(tabreg, aes(gdp, lifexp, size = pop, color = Region)) +
  geom_point(alpha = 0.5) +
  scale_x_log10() +
  theme_bw()
```

Использование цвета наглядно показывает, что африканские страны занимают нижнюю левую часть диаграммы рассеяния с малой величиной ВВП и низкой продолжительностью жизни.

Целесообразно также добавить подписи крупнейших стран мира с населением более 100 млн чел, а также страны, занимающие экстремальные позиции по обеим осям, чтобы понять положение ключевых игроков на диаграмме:
```{r}

tablab = tabreg %>% # табличка для подписей
  filter(
    pop > 1e8 | 
    gdp == min(gdp) | 
    gdp == max(gdp) | 
    lifexp == min(lifexp) | 
    lifexp == max(lifexp)
  )

ggplot(tabreg, aes(gdp, lifexp, color = Region)) +
  geom_point(aes(size = pop), alpha = 0.5) +
  geom_text(data = tablab, 
            aes(label = Country),
            check_overlap = TRUE,
            show.legend = FALSE) + # убрать текст из легенды
  scale_x_log10() +
  theme_bw()
```

Устранение перекрытий подписей можно осуществить, используя геометрию `geom_text_repel()` из пакета __ggrepel__ вместо стандартной `geom_text()`
```{r}
ggplot(tabreg, aes(gdp, lifexp,  color = Region)) +
  geom_point(aes(size = pop), alpha = 0.5) +
  geom_text_repel(data = tablab, 
                  aes(label = Country),
                  box.padding = 0.7,  # зазор вокруг подписи
                  segment.size = 0.2, # толщина линии выноски
                  show.legend = FALSE) + # убрать текст из легенды
  scale_x_log10() +
  labs(label = element_blank()) +
  theme_bw()
```

#### Плотность распределения {#stat_analysis_two_distr_density}

Плотность совместного распределения двух случайных величин представляет собой уже не кривую, а поверхность, которую можно построить с использованием геометрии `geom_density_2d()`. По умолчанию эта геометрия визуализируется в форме изолиний:
```{r}
ggplot(tabreg, aes(gdp, lifexp)) +
  geom_point(alpha = 0.5) +
  geom_density_2d()+
  scale_x_log10() +
  theme_bw()
```

Усилить наглядность представления можно, добавив вспомогательную растровую поверхность плотности распределения (по которой, собственно, и строятся изолинии). Обратите внимание, что для растра используется функция `stat_density()`:
```{r}
ggplot(tabreg, aes(gdp, lifexp)) +
  stat_density_2d(geom = "raster", aes(fill = stat(density)), contour = FALSE) +
  geom_density_2d(color = 'black', size = 0.2) +
  geom_point(alpha = 0.5) +
  scale_fill_gradient(low = "white", high = "red") +
  scale_x_log10() +
  theme_bw()
```

График двумерной плотности распределения показывает, что _мода_ распределения, т.е. наиболее часто встречающийся случай, примерно соответствует странам с продолжительностью жизни 75 лет и ВВП на душу населения \$10000.

В некоторых случаях удобнее оказывается не аппроксимация непрерывной поверхности плотности распределения, а подсчет количества измерений по ячейкам регулярной сетки — квадратным или гексагональным. Такой подход бывает особенно полезен, когда точек измерений очень много и из-за их количества оказывается проблематично разглядеть области их концентрации. Агрегирование данных по ячейкам осуществляется путем применения геометрий `geom_bin2d()` и `geom_hex()`:
```{r}
ggplot(tabreg, aes(gdp, lifexp)) +
  geom_bin2d(bins = 10)+
  geom_point(alpha = 0.5) +
  scale_fill_gradient(low = "white", high = "red") +
  scale_x_log10() +
  theme_bw()

ggplot(tabreg, aes(gdp, lifexp)) +
  geom_hex(bins = 10) +
  geom_point(alpha = 0.5) +
  scale_fill_gradient(low = "white", high = "red") +
  scale_x_log10() +
  theme_bw()
```

### Корреляция и регрессия {#stat_analysis_two_distr_correg}

[Корреляционный анализ](https://bigenc.ru/mathematics/text/2099814) позволяет дать численную характеристику статистической связи между двумя случайными величинами, а [регрессионный анализ](https://bigenc.ru/mathematics/text/3502638) — _моделировать_ эту взаимосвязь посредством построения функции взаимосвязи зависимой и множества независимых переменных.

#### Корреляция {#stat_analysis_two_distr_correg_corr}

[__Коэффициент корреляции__](https://bigenc.ru/mathematics/text/2099711) --- это числовая характеристика совместного распределения двух случайных величин, характеризующая их взаимосвязь. Наиболее часто в статистике употребляется выборочный коэффициент корреляции Пирсона, в котором перебираются все пары соответствующих друг другу значений из рядов $X = \{x_i\}$ и $Y = \{y_i\}$: 
$$
r_{xy} = \frac{\sum_{i=1}^{n}(x_i - \bar x)(y_i - \bar y)}{\sqrt{\sum_{i=1}^{n}(x_i - \bar x)^2} \sqrt{\sum_{i=1}^{n}(y_i - \bar y)^2}},
$$
где $\bar x$ и $\bar y$ соответствуют выборочным средним для $X$ и $Y$. 

Важно помнить, что коэффициент корреляции Пирсона характеризует силу _линейной_ связи между двумя величинами. Поэтому, если наблюдаемая нами картина взаимосвязи носит нелинейный характер, необходимо предварительно линеаризовать ее, то есть выполнить преобразование над переменными, приводящее к получению линейной зависимости. В нашем в случае изучения ВВП на душу населения и продолжительности жизни мы видели, что линеаризация возможна путем логарифмирования показателя ВВП.

Для вычисления коэффициента корреляции Пирсона в __R__ с оценкой уровня значимости используется функция `cor.test()`:
```{r}
cor.test(tabreg$gdp, tabreg$lifexp)
```
Результат теста в данном случае показывает, что коэффициент корреляции с вероятностью 0,95 находится в интервале от 0,56 до 0,73, и его математическое ожидание равно 0,66. 

Проверим, можно ли уточнить эту оценку, выполнив логарифмирование показателя ВВП:
```{r}
cor.test(log(tabreg$gdp), tabreg$lifexp)
```
Видим, что логарифмирование показателя позволяет повысить значение коэффициента корреляции до 0,8. При этом доверительный интервал, заключающий в себя эту величину с вероятностью 0,95 существенно сузился: с 0,17 до 0,11. Очевидно, мы получили более корректную оценку взаимосвязи.

#### Регрессия {#stat_analysis_two_distr_correg_reg}

Для построения статистической модели этой зависимости, позволяющей по значениям независимой переменной вычислять значения зависимой переменной, необходимо провести __регрессионный анализ__. В общем случае кривая регрессии обычно выражается линейной комбинацией набора функций:
$$
y(x) = β_0φ_0(x)+ β_1φ_1(x)+...+ β_mφ_m(x)
$$
Наиболее часто используется _полиномиальная регрессия_, при которой
$$
y(x) = β_0+β_1x+...+ β_mx^m.
$$
В этом случае основная задача регрессионного анализа сводится к поиску неизвестных коэффициентов $β_0,...,β_m$, который осуществляется [методом наименьших квадратов](https://bigenc.ru/mathematics/text/2245173). Результатом этого поиска являются выборочные коэффициенты регрессии $\hat β_0,...,\hat β_m$, которые дают оценку искомых параметров $β_0,...,β_m$. В итоге эмпирическая линия регрессии определяется многочленом
$$
\hat y(x)=\hat β_0+\hat β_1x+...+\hat β_mx_m,
$$
который и служит статистической оценкой неизвестной формы функциональной зависимости между исследуемыми величинами.

Для представления моделей в R существует специальный объект, который называется __формула__. Формула имеет вид `f ~ x + y + ...`, что интерпретируется соответствующими функциями как $f = β_0 + β_1x + β_2y + \dots$

> __Обратите внимание__ на символ _тильды_ (`~`) --- он является отличительной особенностью формулы и интерпретируется как _«зависит от»_.

Вместо переменных в формуле вы можете использовать функции от переменных. Например `log(f) ~ log(x) + sqrt(y)` означает модель $\log f = β_0 + β_1 \log x + β_2 \sqrt y$. Если необходимо выполнить алгебраические преобразования переменных или задать конкретное значение свободного члена, то их необходимо заключить в специальную функцию `I()`: `f ~ log(x) + I(y ^ 2) + I(0)` будет означать модель вида $f = β_1 \log x + β_2 y^2$. 

Для краткой записи полиномиальной зависимости можно использовать вспомогательную функцию `poly()`, которая в качестве второго аргумента принимает степень многочлена. Т.е. `f ~ poly(x, 3)` означает модель вида $f = β_0 + β_1x + β_2x^2 + β_3x^3$.

Оценка параметров линейных моделей осуществляется с помощью функции `lm()`. В нашем случае модель носит простой характер:
```{r}
model = lm(lifexp ~ log(gdp), data = tabreg)
coef(model)
```
Полученные данные говорят нам о том, что уравнение имеет вид $lifexp = 25.13 + 5.26 \log(gdp)$. Чтобы получить подробную сводку о качестве модели, мы можем вызвать `summary()`:
```{r}
summary(model)
```
Результаты оценки говорят о том, что регрессия построена удовлетворительно. [Коэффициент детерминации](https://ru.wikipedia.org/wiki/%D0%9A%D0%BE%D1%8D%D1%84%D1%84%D0%B8%D1%86%D0%B8%D0%B5%D0%BD%D1%82_%D0%B4%D0%B5%D1%82%D0%B5%D1%80%D0%BC%D0%B8%D0%BD%D0%B0%D1%86%D0%B8%D0%B8) (квадрат коэффициента корреляции) равен 0,64.

Для визуализации модели можно извлечь из нее значения используя функцию `fitted()`:
```{r}
df = tibble(lifexp = fitted(model),
            gdp = tabreg$gdp)
              
ggplot(tabreg, aes(gdp, lifexp)) +
  geom_point(alpha = 0.5) +
  geom_line(data = df, aes(gdp, lifexp), color = 'red', size = 1) +
  theme_bw()
```

Если вам нужно только построить линию регрессии, но не находить ее коэффициенты, то вы можете пропустить этап оценки параметров модели и вывести график линейной регрессии средствами __ggplot__, используя геометрию `geom_smooth()` с параметром `method = lm`:
```{r}
ggplot(tabreg, aes(gdp, lifexp)) +
  geom_point(alpha = 0.5) +
  geom_smooth(method = 'lm',
              color = 'red', size = 1) +
  scale_x_log10() +
  theme_bw()
```


#### Локальная регрессия {#stat_analysis_two_distr_correg_loess}

Метод __локальной регрессии__ изначально был разработан для построения кривых регрессии в случае когда зависимость между переменными ведет себя сложным образом и не может быть описана в терминах традиционной линейной и нелинейной регрессии --- глобальных методов. В этом случае область значений независимой переменной $X$ можно покрыть конечным числом отрезков, для каждого из которых далее находят регрессию традиционным методом — как правило, линейную или квадратичную. Данный метод получил название _LOWESS (Locally weighted scatterplot smoothing)_. В дальнейшем эта аббревиатура была редуцирована до _LOESS_. 

В классической постановке метод __LOESS__ реализуется следующим образом [@cleveland:1979]. Пусть дано $n$ точек исходных данных с координатами $x$ (независимая переменная) и $y$ (зависимая). Задается число $0 < \alpha \leq 1$, которое обозначает долю от общего количества точек $n$, выбираемую в окрестности каждой точки для построения регрессии. В абсолютном исчислении количество ближайших точек будет равно $r = [\alpha n]$, где $[\cdot]$ — округление до ближайшего целого.

Тогда вес, который будет иметь каждая $k$-я точка исходных данных в уравнении регрессии для $i$-й точки исходных данных будет определяться по формуле:

$$w_k (x_i) = W\big((x_k - x_i)h_i^{-1}\big),$$

где $h_i$ --- расстояние до $r$-го по близости соседа точки $x_i$, а $W$ --- весовая функция, отвечающая следующим условиям:

1. $W(x) > 0$ если $|x| < 1$;
2. $W(-x) = W(x)$;
3. $W(x)$ невозрастающая функция для $x \geq 0$;
4. $W(x) = 0$ если $|x| \geq 1$.

Одним из стандартных вариантов весовой функции является _"трикубическая"_ функция, определяемая как:

$$
W(x) = \begin{cases}
(1 - |x|^3)^3, & \text{если } |x| < 1, \\
 0, & \text{если } |x| \geq 1.
\end{cases}
$$

Согласно определению весовой функции более близкие к $x_i$ точки оказывают большее влияние на коэффициенты регрессии. Помимо этого за пределами расстояния $h_i$ веса всех точек исходных данных будут обнуляться. 

Сглаженная оценка $\hat{y}_i$ в точке $x_i$ получается в виде полинома степени $d$:

$$\hat{y}_i = \sum_{j=0}^d \hat{\beta}_j (x_i) x_i^j,$$
где коэффициенты $\hat{\beta}_j$ находятся методом наименьших квадратов путем минимизации ошибки:

$$\sum_{k=1}^n w_k (x_i) (y_k - \beta_0 - \beta_1 x_k - ... - \beta_d x_k^d)^2$$
Процедура поиска коэффициентов регрессии посторяется для каждой из $n$ точек. 

В методе __LOESS__ используются степени регрессии $d = 0, 1, 2$. Кубические и более высокие степени полиномов на практике не применяются. При степени равной 0 метод носит название _сглаживающего среднего_. Для построения линии локальной регрессии используйте функцию `geom_smooth()` без параметра `method` или с явным указанием параметра `method = 'loess'`:

```{r}
ggplot(tabreg, aes(gdp, lifexp)) +
  geom_point(alpha = 0.5) +
  geom_smooth() +
  theme_bw()
```

При визуализации линии локальной регрессии __ggplot__ автоматически добавляет доверительные интервалы, показывающие диапазон нахождения искомой регрессионной кривой с вероятностью 0,95. Вы можете регулировать поведение локальной регрессии, задавая параметры `n` (количество ближайших точек $r$), `span` (доля ближайших точек $\alpha$) и `formula` (формула аппроксимируемой зависимости). По умолчанию используется регрессия первой степени (`formula = y ~ x`), значения `n = 80` и `span = 0.75`. Вы можете их изменить, например задать более компактный охват для поиска коэффициентов. В этом случае кривая будет более чувствительна к локальному разбросу элементов выборки:
```{r}
ggplot(tabreg, aes(gdp, lifexp)) +
  geom_point(alpha = 0.5) +
  geom_smooth(span = 0.3) +
  theme_bw()
```

Вместо координат исходных точек для построения регрессии можно использовать и произвольные координаты $X$. В этом случае кривая будет соединять точки, полученные локальной регрессионной оценкой в заданных координатах $X$. Именно этот принцип используется в двумерном (и многомерном) случае. Пусть даны измерения показателя в $N$ исходных точках и задано число $\alpha$ --- сглаживающий параметр. Тогда аппроксимация показателя в каждом узле интерполяции получается путем построения поверхности тренда (см. выше) по $\alpha N$ ближайшим исходным точкам. Как и в одномерном случае, близкие точки будут оказывать более сильное влияние на коэффициенты регрессии, чем удаленные.

Метод _LOESS_ предоставляет широкие возможности настройки благодаря вариативности параметра сглаживания и степени регрессионного полинома.

## Контрольные вопросы и упражнения {#questions_tasks_stat_analysis}

### Вопросы {#questions_stat_analysis}

1. Перечислите названия геометрий __ggplot2__, отвечающих за построение гистограммы и функции плотности распределения.
1. Как работает метод ядерного сглаживания, используемый для аппроксимации функции плотности распределения? Каким критериям должна отвечать ядерная функция?
1. Как совместить на одном графике гистограмму распределения и функцию плотности вероятности? Какой показатель должна отображать гистограмма высотой столбиков?
1. Можно ли при построении графиков статистического характера определить различные веса для измерений? В какой параметр они должны передаваться? Какому критерию должны отвечать веса?
1. С помощью какой геометрии можно построить диаграмму размаха средствами __ggplot2__? Как следует интерпретировать этот график?
1. Как оценить статистическую значимость отличий в средних значениях и дисперсиях двух выборок? Какие тесты можно использовать для этого?
1. Что из себя представляет плотность совместного распределения двух случайных величин? Какая геометрия __ggplot2__ позволяет аппроксимировать ее и нанести на диаграмму рассеяния?
1. С помощью каких геометрий __ggplot2__ можно сгруппировать элементы диаграммы рассеяния ячейками ортогональной и гексагональной сеток? В каких случаях это оказывается полезно?
1. Что такое коэффициент корреляции Пирсона, и какими ограничениями обладает этот показатель?
1. Какая функция позволяет осуществить тест на корреляцию между двумя переменными в __R__?
1. Что позволяет получить регрессионный анализ?
1. Какой вид имеет уравнение регрессии в общем случае?
1. Какой вид регрессии используется чаще всего?
1. С помощью какого метода находят выборочные коэффициенты регрессии?
1. Что такое формула в __R__, и для чего она используется?
1. Как называется символ `~`, и что он означает в формулах?
1. Каким образов в формуле можно указать алгебраическое преобразование переменной?
1. С помощью какой функции осуществляется оценка параметров линейных регрессионных моделей в __R__?
1. Какие функции позволяют извлечь из модели выборочные коэффициенты регрессии, а также смоделированные (_fitted_) значения?
1. Как на основе полученной модели нанести линию регрессии на график __ggplot2__? Опишите последовательность действий.
1. Можно ли нанести линию регрессии на график __ggplot2__, не используя явное построение модели? Какую геометрию и с какими параметрами следует использовать для этого?
1. Что такое локальная регрессия (_LOESS_), и как работает этот метод?
1. Какая геометрия, и с какими параметрами используется для построения линии локальной регрессии на графике __ggplot2__?
1. Что показывает полупрозрачная серая полоса вокруг линии регрессии на графике __ggplot2__?

### Упражнения {#tasks_stat_analysis}

1. Изучите по данным Gapminder такие показатели как [использование энергии](https://docs.google.com/spreadsheets/d/1StdAfQCYzvpSYQMvLfXkZ1FMKqcRh6BbZ6B-bLARIz8/pub) и [выбросы $CO_2$](https://docs.google.com/spreadsheets/d/1RjqGm7RG82GGVf7E4RXPPwFF7O1So6T0SFx2fVfcUJA/pub) на душу населения. Постройте для них гистограммы и кривые плотности распределения. Какое распределение имеют данные показатели? Как изменилось оно с 1990 к 2010 году? Вычислите коэффициент корреляции между этими показателями и постройте регрессионную зависимость (на 2010 год). Дайте оценку статистической значимости полученных результатов.

2. Изучите по данным Gapminder соотношение [доли сельскохозяйсвенных земель в общей площади](https://docs.google.com/spreadsheets/d/1c2RNsNlhv2kMHrqjl8rpFiVkz6iGu56tWlR3ClgaQ4M/pub) и [доле водозабора на сельскохозяйственные нужды](https://docs.google.com/spreadsheets/d/1qpvu21VCsvz-hYu5s1h0fc-3mRpxvsF1Y2M0Ie3d3CU/pub) за 2002 год. Есть ли какая-то зависимость между этими переменными? Что можно сказать о том, как распределены страны мира по этим двум показателям? Постройте по ним диаграммы размаха, сгруппировав по 4, 6 или 8 регионам Gapminder. Дайте оценку статистической значимости отличий в средних значениях и дисперсии между двумя выбранными вами регионами по доле водозабора на сельскохозяйственные нужды.

----
_Самсонов Т.Е._ **Визуализация и анализ географических данных на языке R.** М.: Географический факультет МГУ, `r lubridate::year(Sys.Date())`. DOI: [10.5281/zenodo.901911](https://doi.org/10.5281/zenodo.901911)
----