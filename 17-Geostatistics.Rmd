# (PART) Пространственная статистика {-}

# Геостатистика {#gstat}

```{r setup-gstat, echo = FALSE, purl = FALSE, cache = FALSE, include=FALSE}
options(htmltools.dir.version = FALSE)
library(tidyverse)
library(raster)
library(gstat)
library(stars)
library(tmap)
library(sf)
library(sp)
knitr::opts_knit$set(global.par = TRUE)
```

$$\newcommand{\Var}{\operatorname{Var}}
\newcommand{\Cov}{\operatorname{Cov}}
\newcommand{\E}{\operatorname{E}}$$

[Программный код главы](https://github.com/tsamsonov/r-geo-course/blob/master/code/14-InterpolationGeostatistics.R)

## Введение

**Геостатистика** --- раздел математической статистики, который связан с численным описанием переменных, распределенных в географическом пространстве и, опционально, времени. Наиболее часто инструменты геостатистики используются для решения задачи интерполяции --- восстановления сплошного поля распределения случайной величины по ограниченному множеству данных в точках наблюдений. Однако, геостатистика как научная дисциплина существенно шире. Ее первоочередной задачей является статистическое описание пространственных распределений.

В основе геостатистики лежит широко разработанный математический аппарат. Понимание основ этого аппарата является необходимым условием осмысленного примения геостатистических методов на практике. В настоящей главе мы постараемся сформировать у читателя данное понимание, и показать, как геостатистика работает на практике.

Мир геостатистики базируется на фундаментальных понятиях случайной величины, случайной функции и случайного процесса. Рассмотрим эти понятия.

## Базовые понятия и элементы геостатистики

### Базовые понятия 

Отправной точкой геостатистического анализа является конечное множество точек (локаций), в каждой из которых зафиксировано значение некоторой пространственной переменной. Пространственную и атрибутивную составляющую традиционно разделяют на две компоненты, каждая из которых может быть случайной:

1. Пространственные локации (точки)
  $$\{p_1, p_2, ..., p_n\}$$
2. Данные в этих локациях
  $$\{Z(p_1), Z(p_2), ..., Z(p_n)\}$$

Данные в локациях получаются путем измерений значений пространственно распределенной переменной, или вычисления её значения на основе других данных, которые прямо или косвенно (через другие данные) базируются на прямых или дистанционных наблюдениях. Результаты измерений, как и исходные для расчетов данные, как правило, привязаны ко времени и характеризуют состояние среды на определенный момент. Если описываемое явление является динамическим (изменчивым во времени), результаты наблюдений или расчетов для двух разных моментов времени в общем случае будут различны. Эти различия невозможно _полностью_ описать в аналитическом виде, поскольку природные и социально-экономические процессы формируются неопределенно большим числом факторов. Аналогично этому, невозможно и достоверно предсказать значение пространственной переменной в заданный момент времени. Чтобы работать с такими данными, используются понятия _случайной величины_ и _случайного процесса_.

> __Случайной величиной__ $Z(w)$ называется функция, которая в результате случайного события $w$ принимает некоторое вещественнозначное значение.

Например, при анализе температуры водоема в отдельно взятой точке в толще воды случайной величиной (функцией) является собственно температура, а событием — та совокупность физико-химических условий, которая сложилась в данной точке в данный момент измерений и обусловила наблюдаемое значение температуры.

Отметим, что _элемент случайности вносится именно событием_, которое в природе может быть сформировано сложной и трудно предсказуемой комбинацией факторов, в то время как _случайная величина уже связана с событием некоторой зависимостью_, которую можно описать с помощью аналитических или эмпирических формул.

Случайность можно наблюдать не только в точке, но и по пространству. Например, уровень шума, формируемый автотранспортом в открытой городской среде, меняется непрерывно, и его можно измерить в каждой точке. При этом пространственное распределение величины этого уровня будет в каждый момент времени зависеть от случайного события — размещения автомобилей и уровня шума, производимого каждым из них. Городская среда оказывается полностью заполнена шумовым эфиром, густота которого неодинакова в пространстве и времени. Перемещаясь из точки в точку или ожидая последующего момента времени, находясь в одной точке, мы будем наблюдать разный уровень шума. Состояние этого шумового эфира как единого целого является _случайным процессом_.

Введем общее понятие случайного процесса:

> __Случайный процесс__  это семейство случайных величин, индексированных некоторым параметром $t$

Наиболее часто анализируются одномерные случайные процессы, в которых $t$ --- это время. Классическим примером такого процесса является количество покупателей, находящихся в магазине.

Пространственная статистика изучает случайные процессы, в которых $t$ — это координата точки (обычно на плоскости). Такие процессы характеризуются следующими особенностями:

- в каждой точке $p_i$ существует некоторая _случайная величина_ $Z(p_i)$ — __сечение случайного процесса__

- при изменении точки $p_i$ наблюдаемое значение случайного процесса меняется случайным образом, поскольку определяется оно не только местоположением, но и заранее неизвестным случайным событием.

Для описания случайных процессов в пространстве необходимо сформировать базовую математическую модель, а также определить ее свойства.

### Случайный процесс в пространстве и его моменты

Пусть $p \in \mathbb{R}^k$ --- точка в $k$-мерном Евклидовом пространстве и $Z(p)$ --- __случайная величина__ в точке $p$. Тогда если $p$ меняется в пределах области $D \subset \mathbb{R}^k$ (эта область именуется _индексным множеством_), то формируется __случайный процесс__:

$$\{Z(p) | p \in D\}$$
Вертикальная черта в соответствии с принятой в теории вероятности нотацией означает _условие_. То есть, переменная $p$ ограничена областью $D$. 

Результат наблюдения случайного процесса в точках области $D$ является __реализацией__ случайного процесса:

$$\{z(p) | p \in D\}$$

> В общем случае $D$ и $Z$ случайны и независимы

Случайный процесс, как и случайную величину, можно описать с помощью статистических моментов, таких как математическое ожидание и дисперсия.

__Математическое ожидание__ есть наиболее вероятная реализация случайного процесса:

$$\E[Z(p)]=m(p)$$

Поясним суть математического ожидания СП на следующем примере:

- Пусть дан географический регион, в пределах которого рассматривается поле температуры и его временная изменчивость.
- В каждый момент времени мы имеем непрерывное поле температуры — реализацию случайного процесса.
- Если рассмотреть поведение это поля во временном разрезе (по аналогии с колебаниями волн в пространстве), то получим некое "среднее" поле --- математическое ожидаение случайного процесса.

Если в приведенном примере температура наблюдается посредством сети метеостанций, то в каждый момент времени реализацию СП можно приблизительно восстановить путем выполнения интерполяции по их данным. Осреднив же данные по времени, и снова проинтерполировав их, получим _выборочную среднюю_ поверхность --- оценку мат. ожидания СП.  

__Дисперсия__ есть мера разброса реализаций случайного процесса относительно его математического ожидания:

$$\Var[Z(p)]= \E[Z^2(p)]-m^2(p)$$
Аналогично математическому ожиданию, дисперсия двумерного СП представляет собой _поле распределения_. Величина этого поля каждой точке равна дисперсии сечения СП в этой точке, то есть дисперсии случайной величины. Так же как и в традиционной статистике, вместо дисперсии в расчетах часто используют среднеквадратическое отклонение, поскольку оно выражено в тех же единицах, что и сама случайная величина:

$$\sigma(p)= \sqrt{\Var[Z(p)]}$$

В случае поля температуры можно представить себе объем, ограниченный двумя поверхностями $m(p) + \sigma(p)$ и $m(p) - \sigma(p)$. Расстояние между этими поверхностями в каждой точке $p$ представляет собой среднеквадратическое отклонение случайного процесса.

__Ковариация__  --- это мера линейной зависимости сечений случайного процесса в двух точках $p_1$ и $p_2$:

$$\Cov(p_1,p_2) = \Cov[Z(p_1), Z(p_2)] = \E[Z(p_1)Z(p_2)]-m(p_1)m(p_2)$$

> Для вычисления ковариации необходимость знать математическое ожидание СП. Это условие выполняется далеко не всегда, что связано с тем что как правило приходится иметь дело только с одной его реализацией.

Следует обратить внимание на то, что  _моменты пространственных случайных процессов являются функциями, а не константами_, в отличие от моментов случайных величин.

Давать оценку пространственной структуре явления на основе вычисленных моментов с.п. можно при условии, что он удовлетворяет свойствам __стационарности__ и __эргодичности.__

## Стационарность и эргодичность

### Стационарность

__Стационарность__ _в строгом смысле_ означает что функция распределения множества случайных величин для любой комбинации точек ${x_1, x_2,...,x_k}$ и любого $k < \infty$ остается неизменной при смещении этой комбинации на произвольный вектор $h$:

$$P\{Z(x_1)<z_1,...,Z(x_k)<z_k\} = P\{Z(x_1 + h)<z_1,...,Z(x_k + h)<z_k\}$$

- Стационарность по другому называют __однородностью в пространстве__, подразумевая что явление ведет себя одинаковым образом в любой точке пространства, как бы повторяет само себя.

- Если СФ стационарна, все ее моменты будут инвариантны относительно сдвигов (то есть будут постоянны), а это означает что для их оценки можно использовать ограниченную в пространстве область.

В реальности подобного рода «идеальное» поведение встречается крайне редко, поэтому используют более слабое предположение о стационарности второго порядка.

Случайная функция имеет имеет __стационарность второго порядка__, если для любых точек $x$ и $x+h$ в $R^k$

$$\begin{cases}
  \E[Z(x)] = m \\
  \E[(Z(x)-m)(Z(x+h)-m)] = \Cov(h)
\end{cases}$$

Данные условия означают, что математическое ожидание СФ постоянно, а ковариация зависит только от вектора $h$ между точками и не зависит от их абсолютного положения.

Если ковариация также не зависит от направления, а только от расстояния между точками, то $h$ вырождается в скаляр, а такая случайная функция является _изотропной стационарной_.

### Эргодичность

Стационарная случайная функция $Z(x,w)$ называется __эргодической__, если ее среднее по области $V \subset R^k$ сходится к математическому ожиданию $m(w)$ при стремлении $V$ к бесконечности:

$$\lim_{V \rightarrow \infty} \frac{1}{|V|}\int_{V} Z(x,w)dx = m(w),$$
где $|V|$ обозначает _меру_ области $V$ (площадь, объем). Предполагается что сама область $V$ растет во всех направлениях, и предел ее роста не зависит от ее формы.

> Следствием эргодичности является то, что среднее по всем возможным реализациям равно среднему отдельной безграничной в пространстве реализации.

Смысл эргодичности можно пояснить на следующем примере. Пусть дан кувшин с песком, в котором необходимо определить долю объема, занятую содержимым. Проведем следующий эксперимент:

- Зафиксируем некоторую точку $x$ в системе отсчета, привязанной к кувшину, и будем его встряхивать бесконечное число раз, каждый раз фиксируя, оказалась ли точка $x$ внутри песчинки (записываем 1) или же попала в свободное между ними пространство (записываем 0)
- Из серии подобных экспериментов мы сможем оценить среднее значение индикаторной функции $I(x,w)$, которое равно вероятности попадания зерна в точку $x$, и которое не зависит от $x$.
- Эта вероятность и будет равна доли объема кувшина, занятой песком.

Аналогичный  результат можно получить, если теперь зафиксировать кувшин, а точку $x$ выбирать каждый раз случайным образом. Однако в первом случае берется среднее по реализациям, а во втором среднее по пространству.

## Геостатистическое оценивание

###  Простой кригинг

Для оценки в точке $Z_0 = Z(p_0)$ по $N$ измерениям $Z_1, ..., Z_N$ ищутся коэффициенты $\lambda$ следующего выражения:

$$Z^* = \sum_{i} \lambda_i Z_i + \lambda_0,$$
где константа $\lambda_0 = \lambda(p_0)$ и веса $\lambda_i$ подобираются в точке $p_0$ таким образом, что минимизируется среднеквадратическая ошибка:

$$\E[Z^* - Z_0]^2,$$
то есть, математическое ожидание квадрата отклонения оценки от теоретического значения.

> Согласно традиции, принятой в литературе по геостатистике, оценка в точке $p_0$ обозначается звездочкой ($Z^*$), а истинное (неизвестное) значение нулевым индексом ($Z_0$).

> В целях уменьшения количества скобок мы используем нотацию $\operatorname{E}[Z^* - Z_0]^2 = \operatorname{E}\big[(Z^* - Z_0)^2\big]$

Теоретическое значение $Z_0$, входящее в формулу ошибки, не известно. Однако, как будет показано далее, его знание и не требуется, поскольку _ищется не сам квадрат отклонения, а его математическое ожидание_. 

Используя соотношение $\Var[X] = \E[X^2] - (\E[X])^2$, выразим среднюю квадратическую ошибку как:

$$\E[Z^* - Z_0]^2 = \Var[Z^* - Z_0] + \big(\E[Z^* - Z_0]\big)^2$$

Поскольку дисперсия нечувствительна к сдвигам, изменение константы $\lambda_0$ влияет только на компоненту $\E[Z^* - Z_0]$. Приравняем ее нулю:

$$\E[Z^* - Z_0] = \E\Big[\sum_{i} \lambda_i Z_i + \lambda_0 - Z_0\Big] = 0$$

Используя свойства математического ожидания, выразим из этого выражения $\lambda_0$:

$$\lambda_0 = - \E\Big[\sum_{i} \lambda_i Z_i - Z_0\Big] = \E[Z_0] - \sum_{i} \lambda_i \E[Z_i] = m_0 - \sum_i \lambda_i m_i,$$

где $m_i$ --- теоретически известные значения мат. ожидания случайной функции в точках исходных данных $p_i$, $m_0$ --- теоретически известное мат. ожидание случайной функции в оцениваемой точке $p_0$.

Имея:

$$Z^* = \sum_{i} \lambda_i Z_i + \lambda_0,\\
\lambda_0 = m_0 - \sum_i \lambda_i m_i,$$

Получаем:

$$Z^* = \sum_{i} \lambda_i Z_i + m_0 - \sum_i \lambda_i m_i = m_0 + \sum_{i} \lambda_i (Z_i - m_i)$$
Поскольку математические ожидания $m_0$ и $m_i$ предполагаются известными, величину $Z(p)$ можно заменить величиной $Y(p) = Z(p) - m(p)$ в обеих частях уравнения (это эквивалентно вычитанию известного тренда из исходных измерений):

$$
Z^* = m_0 + \sum_{i} \lambda_i (Z_i - m_i),\\
\underbrace{Z^* - m_0}_{Y^*} = \sum_{i} \lambda_i \underbrace{(Z_i - m_i)}_{Y_i},\\
Y^* =  \sum_{i} \lambda_i Y_i.
$$
Это означает, что оценку $Z^*$ можно заменить оценкой $Y^*$ и прибавлением к ней среднего значения $m_0$:

$$Z^* = m_0 + \sum_{i} \lambda_i Y_i,$$

Обратим внимание на то, что $\E[Y^* - Y_0] = \underbrace{\E[Y^*]}_0 - \underbrace{\E[Y_0]}_0 = 0$ по определению $Y(p)$. В этом случае также $\lambda_0 = \underbrace{\E[Y_0]}_0 - \sum_{i} \lambda_i \underbrace{\E[Y_i]}_0 = 0$ . 

Чтобы не загромождать дальнейшее изложение формулами, примем, что $Z^* := Y^*$, предполагая, что исходная величина уже центрирована относительно мат. ожидания, и к результату вычислений его надо прибавить.

Поскольку $\E[Z^* - Z_0] = 0$, среднеквадратическую ошибка будет равна дисперсии:

$$\E[Z^* - Z_0]^2 = \Var[Z^* - Z_0]$$
Выразим дисперсию разностей в терминах статистических моментов исходной функции. Для этого воспользуемся следующими свойствами дисперсии и ковариации:

- $\Var[X + Y] = \Var[X] + \Var[Y] + 2\Cov[X, Y]$; 
- $\Var[-X] = \Var[X]$;
- $\Cov[X, -Y] = -\Cov[X, Y]$. 

Используя эти свойства, получаем:

$$\E[Z^* - Z_0]^2 = \Var[Z^* - Z_0] = \Var[Z^*] + \Var[Z_0] - 2\Cov[Z^*, Z_0].$$

Чтобы минимизировать данное выражение, необходимо раскрыть содержание трёх его компонент. Для этого нам понадобится следующая теорема, позволяющая выразить ковариацию двух линейных комбинаций случайных величин через ковариацию самих исходных случайных величин:  

```{theorem}
Пусть $X_1,\ldots, X_n$ случайные величины, а $Y_1 = \sum\limits_{i=1}^n a_i X_i,\; Y_2 = \sum\limits_{j=1}^m b_j X_j$ — их две произвольные линейные комбинации. Тогда:

$$\Cov[Y_1,Y_2] = \sum\limits_{i=1}^n\sum\limits_{j=1}^m a_i b_j \Cov[X_i,X_j].$$
```

Используя результат этой теоремы, а также тот факт, что $\Var[X] = \Cov[X, X]$, распишем каждую компоненту вышеприведенного выражения:

- $\Var[Z^*] = \Cov[Z^*, Z^*] = \Cov\Big[\sum_{i} \lambda_i Z_i, \sum_{j} \lambda_j Z_j\Big] =\\\sum_{i}\sum_{j} \lambda_i \lambda_j \Cov[Z_i, Z_j] = \sum_{i}\sum_{j} \lambda_i \lambda_j \sigma_{ij},$

- $\Var[Z_0] = \Cov[Z_0, Z_0] = \sigma_{00},$

- $\Cov[Z^*, Z_0] = \Cov\Big[\sum_{i} \lambda_i Z_i, Z_0\Big] = \sum_{i} \lambda_i \Cov[Z_i, Z_0] = \sum_{i} \lambda_i \sigma_{i0},$

где $\sigma_{ij}$ --- ковариация случайных величин в точках $p_i$ и $p_j$, $\sigma_{i0}$ --- ковариация случайных величин в точках $p_i$ и оцениваемой точке $p_0$, $\sigma_{00}$ --- дисперсия в точке $p_0$.

Используя полученные соотношения, выражение для ошибки

$$\E[Z^* - Z_0]^2 = \Var[Z^*] + \Var[Z_0] - 2\Cov[Z^*, Z_0]$$

можно представить следующим образом:

$$\E[Z^* - Z_0]^2 = \sum_{i}\sum_{j} \lambda_i \lambda_j \sigma_{ij} - 2 \sum_{} \lambda_i \sigma_{i0} + \sigma_{00}$$

Для нахождения минимума этой квадратичной функции необходимо приравнять нулю ее производные по основной переменной $\lambda$. Выберем в качестве «жертвы» коэффициенты с индексом $i$:

$$\frac{\partial}{\partial \lambda_i} \E[Z^* - Z_0]^2 = 2 \sum_{j} \lambda_j \sigma_{ij} - 2 \sigma_{i0} = 0$$

Таким образом, система уравнений __простого кригинга__ для точки $Z_0$ имеет вид:

$$\color{red}{\boxed{\color{blue}{\sum_{j} \lambda_j \sigma_{ij} = \sigma_{i0}\color{gray}{,~i = 1,...,N}}}}$$

Полученные __уравнения простого кригинга носят чисто теоретический характер__, поскольку предполагают знание ковариационной матрицы $\Sigma = \{\sigma_{ij}\}$ случайного процесса. Тем не менее, на их основе удобно выводить уравнения обычного (ординарного) кригинга, в котором подобное знание уже не требуется.

### Дисперсия простого кригинга

Существует возможность оценить в каждой точке не только величину показателя, но также дисперсию оценки (в случае постоянного мат. ожидания — среднеквадратическую ошибку).

Для этого необходимо коэффициенты $\lambda_i$, полученные из системы уравнения простого кригинга

$$\sum_{j} \lambda_j \sigma_{ij} = \sigma_{i0}$$

подставить в выражение среднеквадратической ошибки

$$\Var[Z^* - Z_0] = \sum_{i}\sum_{j} \lambda_i \lambda_j \sigma_{ij} - 2 \sum_{} \lambda_i \sigma_{i0} + \sigma_{00}$$

Умножим обе части каждого уравнения простого кригинга на $\lambda_i$ и просуммируем все уравнения по $i$:

$$\sum_{j} \lambda_j \sigma_{ij} = \sigma_{i0}~\Bigg|\times \lambda_i\\
\color{red}{\sum_{i}\sum_{j} \lambda_i \lambda_j \sigma_{ij}} = \color{blue}{\sum_{i}\lambda_i\sigma_{i0}}$$

Заметим, что левая часть уравнения присутствует в выражении среднеквадратической ошибки:

$$\Var[Z^* - Z_0] = \color{red}{\sum_{i}\sum_{j} \lambda_i \lambda_j \sigma_{ij}} - 2 \sum_{} \lambda_i \sigma_{i0} + \sigma_{00}$$

Выполним соответствующую замену $\sum_{i}\sum_{j} \lambda_i \lambda_j \sigma_{ij}$ на $\sum_{i}\lambda_i\sigma_{i0}$:

$$\Var[Z^* - Z_0] = \color{blue}{\sum_{i}\lambda_i\sigma_{i0}} - 2 \sum_{} \lambda_i \sigma_{i0} + \sigma_{00}$$

Отсюда получаем выражение для дисперсии (ошибки) простого кригинга:

$$\color{red}{\boxed{\color{blue}{\sigma_{SK} = \Var[Z^* - Z_0] = \sigma_{00} - \sum_{i}\lambda_i\sigma_{i0}}}}$$

### Стационарность приращений

Стационарность второго порядка требует знания математического ожидания для вычисления ковариации. 

В ряде случаев оценить математическое ожидание оказывается невозможно (оно может не существовать) или же оно действительно оказывается непостоянным.

Тогда пользуются еще более мягкой формой стационарности   __стационарностью приращений__, при которой стационарной предполагается не сама с.ф. $Z(x)$, а производная от нее функция: 

  $$Y_h(x) = Z(x+h)-Z(x)$$

Функция $Z(x)$, обладающая таким свойством, называется подчиняющейся _внутренней гипотезе_.

У функции $Y_h(x) = Z(x+h)-Z(x)$ должны существовать математическое ожидание и дисперсия __приращений__:

$$\begin{cases}
\E[Z(x+h)-Z(x)] = \langle a,h \rangle \\
\Var[Z(x+h)-Z(x)] = 2\gamma(h)
\end{cases}$$

- $\langle a,h \rangle$ обозначает линейный тренд $a$ при заданном векторе $h$ (_математическое ожидание разности значений_), который варажется через скалярное произведение: $\langle a,h \rangle = \sum_i a_i h_i$

- $\gamma(h)$ — дисперсия приращений, называемая _вариограммой_

Если процесс подчиняется гипотезе стационарности второго рода $\E[Z(x)] = m$, то $\E[Z(x+h)-Z(x)] = \E[Y_h(x)] = 0$ и вариограмму можно выразить следующим образом:

$$2\gamma(h) = \Var[Z(x+h)-Z(x)] = \Var[Y_h(x)] \\=\E\big[Y_h(x)\big]^2 - \Big(\E\big[Y_h(x)\big]\Big)^2 \\=\E\big[Y_h(x)\big]^2 = \E\big[Z(x+h)-Z(x)\big]^2$$

Таким образом, наиболее распространенная в геостатистике гипотеза подчиняется следующим условиям:

$$\begin{cases}
\E\big[Z(x)\big] = m\\
\E\big[Z(x+h)-Z(x)\big] = 0 \\
\E\big[Z(x+h)-Z(x)\big]^2 = 2\gamma(h)
\end{cases}$$

- Эти условия позволяют избавиться от необходимости знания среднего значения и дисперсии случайной функции и использовать для вычислений вариограмму. 

- Чтобы модифицировать соответствующим образом уравнения простого кригинга, необходимо знать связь между ковариацией и вариограммой.

### Переход от ковариации к вариограмме

#### Предварительные условия

Для того чтобы функция могла считаться ковариацией, необходимо, чтобы дисперсия, вычисленная на ее основе, была положительной:

$$\Var \Bigg[\sum_{i=1}^N \lambda_i Z(x_i)\Bigg] = \Cov\Bigg[\sum_{i=1}^N \lambda_i Z(x_i), \sum_{j=1}^N \lambda_j Z(x_j)\Bigg] = \\=\sum_{i=1}^N \sum_{j=1}^N \lambda_i \lambda_j \Cov\big[Z(x_i), Z(x_j)\big] \\= \sum_{i=1}^N \sum_{j=1}^N \lambda_i \lambda_j C(x_j - x_i)$$

> Функция $C(h)$, для которой при любых значениях $N$, $x_i$ и $\lambda_i$ выражение $\sum_{i=1}^N \sum_{j=1}^N \lambda_i \lambda_j C(x_j - x_i)$ принимает неотрицательные значения, называется __положительно определенной__.

Если функция отвечает внутренней гипотезе, нет гарантий, что ее ковариация существует и ограничена. В этом случае можно оценить дисперсию суммы случайных функций через дисперсию приращений, наложив дополнительное условие $\sum_{i=1}^N \lambda_i = 0$. Учитывая что $\sum_{i=1}^N \lambda_i Z(x_0) = Z(x_0) \sum_{i=1}^N \lambda_i = 0$,  имеем:

$$\sum_{i=1}^N \lambda_i Z(x_i) = \sum_{i=1}^N \lambda_i Z(x_i) - \sum_{i=1}^N \lambda_i Z(x_0) = \sum_{i=1}^N \lambda_i \big[Z(x_i) - Z(x_0)\big]$$

> Линейные комбинации, отвечающие условию $\sum_{i=1}^N \lambda_i = 0$, называются __допустимыми линейными комбинациями__.

#### Вывод выражения для ковариации через вариограмму

Рассмотрим ковариацию двух линейных комбинаций с.ф.:

$$\Cov \Bigg[\sum_{i=1}^N \lambda_i Z(x_i), \sum_{j=1}^M \mu_j Z(x_j) \Bigg] = \sum_{i=1}^N \sum_{j=1}^M \lambda_i \mu_j C(x_j - x_i)$$

Используя правило $\Cov[X + \alpha, Y + \beta] = \Cov[X, Y]$, введем условное начало координат:

$$\sum_{i=1}^N \sum_{j=1}^M \lambda_i \mu_j C(x_j - x_i) =\\
=\sum_{i=1}^N \sum_{j=1}^M \lambda_i \mu_j \Cov \big[Z(x_i) - Z(x_0), Z(x_j) - Z(x_0)\big]$$

Распишем ковариацию через математические ожидания, учитывая, что, согласно гипотезе, $\E\big[Z(x+h)-Z(x)\big] = 0$:

$$\Cov \big[Z(x_i) - Z(x_0), Z(x_j) - Z(x_0)\big] =\\
= \E\big[Z_i - Z_0\big]\big[Z_j - Z_0\big] - \E\big[Z_i - Z_0\big] E\big[Z_j - Z_0\big] = \\
= \E\big[Z_i - Z_0\big]\big[Z_j - Z_0\big]$$

Обратим внимание, что произведение приращений можно выразить через квадраты приращений:

$$\color{blue}{(Z_j - Z_i)^2} = \big[(Z_j - Z_0) - (Z_i - Z_0)\big]^2 = \\ = \color{blue}{(Z_i - Z_0)^2} - 2\color{red}{(Z_i - Z_0)(Z_j - Z_0)} + \color{blue}{(Z_j - Z_0)^2}$$

Имеем:

$$ (Z_i - Z_0)(Z_j - Z_0) = \frac{1}{2} \Big[(Z_i - Z_0)^2 + (Z_j - Z_0)^2 - (Z_j - Z_i)^2\Big]$$
Учитывая, что $\E\big[Z(x+h)-Z(x)\big]^2 = 2\gamma(h)$, подставим это выражение в формулу вычисления ковариации:

$$\Cov \big[Z(x_i) - Z(x_0), Z(x_j) - Z(x_0)\big] = \E\big[Z_i - Z_0\big]\big[Z_j - Z_0\big] = \\
= \frac{1}{2} \E \Big[(Z_i - Z_0)^2 + (Z_j - Z_0)^2 - (Z_j - Z_i)^2\Big] = \\
= \gamma(x_i - x_0) + \gamma(x_j - x_0) - \gamma(x_j - x_i)$$

Подставим полученное выражение в двойную сумму:

$$\sum_{i=1}^N \sum_{j=1}^M \lambda_i \mu_j \Cov \big[Z(x_i) - Z(x_0), Z(x_j) - Z(x_0)\big] = \\ 
= \sum_{i=1}^N \sum_{j=1}^M \lambda_i \mu_j \big[\gamma(x_i - x_0) + \gamma(x_j - x_0) - \gamma(x_j - x_i)\big] = \\
= \underbrace{\color{red}{\sum_{i=1}^N \sum_{j=1}^M \lambda_i \mu_j \gamma(x_i - x_0)}}_0 + \underbrace{\color{blue}{\sum_{i=1}^N \sum_{j=1}^M \lambda_i \mu_j \gamma(x_j - x_0)}}_0 - \sum_{i=1}^N \sum_{j=1}^M \lambda_i \mu_j \gamma(x_j - x_i) = \\
= - \sum_{i=1}^N \sum_{j=1}^M \lambda_i \mu_j \gamma(x_j - x_i),$$

Компоненты суммы, содержащие $x_0$, будут равны нулю, поскольку они содержат коэффициенты с индексами, не участвующими в вычислении $\gamma$ — суммирование по этим индексам можно вынести наружу:

- $\color{red}{\sum_{i=1}^N \sum_{j=1}^M \lambda_i \mu_j \gamma(x_i - x_0)} = \underbrace{\sum_{j=1}^M \mu_j}_0 \sum_{i=1}^N \lambda_i \gamma(x_i - x_0) = 0;$
- $\color{blue}{\sum_{i=1}^N \sum_{j=1}^M \lambda_i \mu_j \gamma(x_j - x_0)} = \underbrace{\sum_{i=1}^N \lambda_i}_0 \sum_{j=1}^M \mu_i \gamma(x_j - x_0) = 0,$

где соотношения $\sum_{i=1}^N \lambda_i = 0$ и $\sum_{j=1}^M \mu_j = 0$ соблюдаются в силу того, что мы работаем с _допустимыми линейными комбинациями_.

Согласно полученным выкладкам у нас появляется возможность сравнить выражение для ковариации двух линейных комбинаций случайных величин для стационарного случая и внутренней гипотезы:

$$\Cov \Bigg[\sum_{i=1}^N \lambda_i Z(x_i), \sum_{j=1}^M \mu_j Z(x_j) \Bigg] = \underbrace{\sum_{i=1}^N \sum_{j=1}^M \lambda_i \mu_j \sigma(x_j - x_i)}_{\texttt{стационарный случай}} = \underbrace{- \sum_{i=1}^N \sum_{j=1}^M \lambda_i \mu_j \gamma(x_j - x_i)}_{\texttt{внутренняя гипотеза}}$$

Видно, данные выражения отличаются лишь знаком (в случае внутренней гипотезы появляется минус) и вычисляемой функцией (ковариация заменяется на вариограмму при переходе к внутренней гипотезе).

Таким образом, получаем важнейший вывод, необходимый для решения системы уравнений обычного кригинга, рассматриваемых в следующем параграфе:

> __При соблюдении внутренней гипотезы в уравнениях кригинга можно принять__ $\sigma_{ij} = -\gamma_{ij}$

Следствием этого также является то, что дисперсия _допустимой линейной комбинации_ может быть выражена через вариограмму:

$$\Var \Bigg[\sum_{i=1}^N \lambda_i Z(x_i)\Bigg] = - \sum_{i=1}^N \sum_{j=1}^N \lambda_i \lambda_j \gamma(x_j - x_i)$$

Функция $G(h)$, для которой при условии и $\sum_{i=1}^N \lambda_i = 0$ выражение $\sum_{i=1}^N \sum_{j=1}^N \lambda_i \lambda_j G(x_j - x_i)$ принимает неотрицательные значения, называется __условно положительно определенной__.

> $-\gamma(h)$ — условно положительно определенная ф.

### Обычный кригинг

Пусть дано неизвестное среднее $m(x) = a_0$. Необходимо произвести линейную оценку $Z^* = \sum_{i} \lambda_i Z_i + \lambda_0$. Выразим среднюю квадратическую ошибку:

$$\E\big[(Z^* - Z_0)^2\big] = \Var[Z^* - Z_0] + \big(\E[Z^* - Z_0]\big)^2 =\\
= \Var[Z^* - Z_0] + \Bigg[\E\bigg[\sum_{i} \lambda_i Z_i + \lambda_0\bigg] - \E\big[Z_0\big] \Bigg]^2 =\\= \Var[Z^* - Z_0] + \Bigg[\underbrace{\E[\lambda_0]}_{\lambda_0} + \sum_{i} \lambda_i \underbrace{\E\big[Z_i\big]}_{a_0} - \underbrace{\E\big[Z_0\big]}_{a_0} \Bigg]^2 =\\= \Var[Z^* - Z_0] + \Bigg[\lambda_0 + \bigg(\sum_i \lambda_i - 1 \bigg) a_0 \Bigg]^2$$

- Только компонента сдвига $\E[Z^* - Z_0]$ содержит $\lambda_0$, однако, в отличие от случая простого кригинга, мы не можем минимизировать ее, не зная $a_0$.
- Единственный способ избавиться от $a_0$ заключается в том, чтобы наложить дополнительное условие $\sum \lambda_i - 1 = 0$

Минимизируем ранее введенную функцию ошибки:

$$\Var[Z^* - Z_0] = \sum_{i}\sum_{j} \lambda_i \lambda_j \sigma_{ij} - 2 \sum_{} \lambda_i \sigma_{i0} + \sigma_{00}$$

Для этого, с учетом дополнительного условия  $\sum \lambda_i -1 = 0$ применим метод множителей Лагранжа и построим вспомогательную функцию:

$$Q = \Var[Z^* - Z_0] + 2\mu \bigg(\sum_i \lambda_i - 1 \bigg),$$

где $\mu$ -- неизвестный множитель Лагранжа.

Для минимизации функции приравняем нулю ее частные производные:

$$\begin{cases}\frac{\partial Q}{\partial \lambda_i} = 2 \sum_j \lambda_j \sigma_{ij} - 2 \sigma_{i0} + 2\mu = 0,~i = 1,...,N,\\
\frac{\partial Q}{\partial \mu} = 2\bigg(\sum_i \lambda_i - 1 \bigg) = 0
\end{cases}$$

Имеем $N + 1$ уравнений с $N + 1$ неизвестными:

$$\begin{cases}\sum_j \lambda_j \sigma_{ij} + \mu = \sigma_{i0},~i = 1,...,N,\\
\sum_i \lambda_i = 1
\end{cases}$$

Заменяя ковариацию на вариограмму, согласно выводу, полученному в предыдущем параграфе, получаем __систему уравнений обычного кригинга__:

$$\color{red}{\boxed{\color{blue}{\begin{cases}\sum_j \lambda_j \gamma_{ij} - \mu = \gamma_{i0},\color{gray}{~i = 1,...,N,}\\
\sum_i \lambda_i = 1
\end{cases}}}}$$

> Обычный (или ординарный) кригинг — наиболее часто используемый геостатистический метод интерполяции.

### Дисперсия обычного кригинга

Вывод формулы для оценки дисперсии обычного кригинга выполняется аналогично случаю простого кригинга. Умножим $N$ первых уравнений на $\lambda_i$, просуммируем их по $i$:

$$\sum_j \lambda_j \gamma_{ij} - \mu = \gamma_{i0}~\Bigg|\times \lambda_i$$

Учтя дополнительное условие $\sum_i \lambda_i = 1$, получаем выражение для оценки дисперсии (ошибки) обычного кригинга:

$$\color{red}{\boxed{\color{blue}{\sigma_{OK} = Var[Z^* - Z_0] = \sum_{i}\lambda_i\gamma_{i0} - \mu}}}$$

### Универсальный кригинг

В методе универсального кригинга осуществляется декомпозиция переменной $Z(x)$ в виде следующей суммы:

$$Z(x) = m(x) + Y(x)$$

- $m(x)$ — __дрифт__ (_drift_), гладкая детерминированная функция, описывающая _систематическую_ составляющую пространственной изменчивости явления;

- $Y(x)$ - __остаток__ (_residual_), случайная функция с нулевым математическим ожиданием, описывающая _случайную_ составляющую пространственной изменчивости явления;

> Декомпозиция любого явления на дрифт и остаток зависит от масштаба рассмотрения явления. 

Метод универсального кригинга используется, когда математическое ожидание случайного процесса непостоянно по пространству. Это позволяет интерполировать данные, в которых присутствует тренд.

В предположении, что м.о. имеет функциональную зависимость от других процессов в точке $x$, вводится следующая модель:

$$m(x) = \sum_{k=0}^{K} a_k f^k(x),$$

где $f^k(x)$ — известные _базисные функции_, а $a_k$ — фиксированные для точки $x$, но неизвестные коэффициенты.

- Обычно первая базисная функция при $k = 0$ представляет собой константу, равную 1. Это позволяет включить в модель случай постоянного м.о.

- Если среднее зависит только от местоположения, то оставшиеся функции $f^k(x), k > 0$, как правило, представляют собой одночлены от координат (например, для двумерного случая $f^2(p) = x^2 + y^2$)

- Коэффициенты $a_k$ могут меняться в зависимости от $x$, но обязательно медленно, чтобы их можно было считать постоянными в окрестности $x$.

В качестве дрифта $m(x)$ можно использовать не только функцию от местоположения, но также значения внешней переменной — __ковариаты__.

Например, количество осадков можно связать с высотой точки $H(x)$ следующей моделью:

$$Z(x) = a_0 + a_1 H(x) + Y(x)$$

С статистической точки зрения это линейная регрессия, в которой остатки коррелированы (автокоррелированы).

> В литературе данный метод называют также __регрессионным кригингом__ (_regression kriging_), а оценка дрифта — __пространственной регрессией__ (_spatial regression_).

Для вывода уравнений рассмотрим среднеквадратическую ошибку:
$$\E[Z^* - Z_0]^2 = \Var[Z^* - Z_0] + \big(\E[Z^* - Z_0]\big)^2$$

Используя введенную модель дрифта $m(x) = \sum_{k=0}^{K} a^k f^k(x)$ распишем выражение для мат.ожидания приращений:

$$\E[Z^* - Z_0] = \E[Z^*\big] - \E[Z_0] = \\
\sum_i \lambda_i \sum_k a_k f_i^k - \sum_k a_k f_0^k = \sum_k a_k \Bigg(\sum_i \lambda_i f_i^k - f_0^k\Bigg)$$

Чтобы минимизировать $\E[Z^* - Z_0]$ независимо от коэффициентов $a_k$, достаточно в вышеприведенной формуле приравнять нулю выражение в скобках. Отсюда имеем:

$$\sum_i \lambda_i f_i^k = f_0^k,~k = 0, 1, ..., K.$$

Эти условия называются __условиями универсальности__. Отсюда идет название метода — _универсальный кригинг_

> Условия универсальности гаранируют, что оценка $Z^*$ является __несмещенной__ для _любых_ значений $a_k$.

Минимизируем ранее введенную функцию ошибки:

$$\Var[Z^* - Z_0] = \sum_{i}\sum_{j} \lambda_i \lambda_j \sigma_{ij} - 2 \sum_{} \lambda_i \sigma_{i0} + \sigma_{00}$$

Для этого, с учетом дополнительного условия  $\sum_i \lambda_i f_i^k = f_0^k$ применим метод множителей Лагранжа и построим вспомогательную функцию:

$$Q = \Var[Z^* - Z_0] + 2 \sum_{k=0}^K \mu_k \Bigg[ \sum_i \lambda_i f_i^k - f_0^k\Bigg],$$

где $\mu_k,~k = 0, 1, ..., K$ представляют $K + 1$ дополнительных неизвестных, множители Лагранжа.

Для минимизации функции приравняем нулю ее частные производные:

$$\begin{cases}\frac{\partial Q}{\partial \lambda_i} = 2 \sum_j \lambda_j \sigma_{ij} -2 \sigma_{i0} + 2 \sum_k \mu_k f_i^k = 0,\color{gray}{~i = 1,...,N,}\\
\frac{\partial Q}{\partial \mu} = 2\bigg[\sum_i \lambda_i f_i^k - f_0^k \bigg] = 0\color{gray}{,~k = 0, 1,..., K.}
\end{cases}$$

Имеем систему из $N + K + 1$ уравнений с $N + K + 1$ неизвестными:

$$\begin{cases}\sum_j \lambda_j \sigma_{ij} + \sum_k \mu_k f_i^k = \sigma_{i0},~i = 1,...,N,\\
\sum_i \lambda_i f_i^k = f_0^k,~k = 0, 1,..., K.
\end{cases}$$

Заменяя ковариацию на вариограмму, получаем __систему уравнений универсального кригинга__:

$$\color{red}{\boxed{\color{blue}{\begin{cases}\sum_j \lambda_j \gamma_{ij} - \sum_k \mu_k f_i^k = \gamma_{i0},\color{gray}{~i = 1,...,N,}\\
\sum_i \lambda_i f_i^k = f_0^k\color{gray}{,~k = 0, 1,..., K.}
\end{cases}}}}$$

### Дисперсия универсального кригинга

Вывод формулы для оценки дисперсии универсального кригинга выполняется аналогично случаю обычного кригинга. Умножим $N$ первых уравнений на $\lambda_i$, просуммируем их по $i$:

$$\sum_j \lambda_j \gamma_{ij} - \sum_k \mu_k f_i^k = \gamma_{i0} ~ \Bigg|\times \lambda_i$$

Учтя дополнительное условие $\sum_i \lambda_i f_i^k = f_0^k$, получаем выражение для оценки дисперсии (ошибки) универсального кригинга:

$$\color{red}{\boxed{\color{blue}{\sigma_{UK} = \E[Z^* - Z_0]^2 = \sum_{i}\lambda_i\gamma_{i0} - \sum_k \mu_k f_0^k}}}$$

### Кросс-валидация

Значение переменной $Z(x)$ оценивается в каждой точке $x_i$ по данным в соседних точках $Z(x_j), ~ j \neq i$ как если бы $Z(x_i)$ было неизвестно. 

В каждой точке вычисляется оценка кригинга $Z_{-i}^*$ и соответствующая дисперсия кригинга $\sigma_{Ki}^2$. Поскольку значение $Z_i = Z(x_i)$ известно, мы можем вычислить:

- __Ошибку кригинга__ $E_i = Z_{-i}^* - Z_i$
- __Стандартизированную ошибку__ $e_i = E_i / \sigma_{Ki}$

Если $\gamma(h)$ — теоретическая вариограмма, то $E_i = Z_{-i}^* - Z_i$ — случайная величина с м.о. = $0$ и дисперсией $\sigma_{K_i}^2$, а $e_i$ имеет м.о. = $0$ и дисперсию, равную $1$.

Стандартно анализируются следующие карты и графики:

- _Карта стандартизированных ошибок_ $e_i$. Стационарность ошибок, отсутствие эффекта пропорциональности.

- _Гистограмма стандартизированных ошибок_ $e_i$. Нормальность распределения, отсутствие аномалий.

- _Диаграмма рассеяния_ $(Z_{-i}^*, Z_i)$. Сглаживающий эффект, соответствие оценки и реального значения.

- _Диаграмма рассеяния_ $(Z_{-i}^*, e_i)$. Независимость (ортогональность) оценки и ошибки.


### Вариограмма

__Вариограмма__ _(полувариограмма)_  дисперсия разности значений в точках как функция от их взаимного положения:

$$\gamma (h) = \gamma(x, x + h) = \E\big[Z(x + h)-Z(x)\big]^2$$

Для $N$ точек, разделенных вектором $\mathbf{h}$:

$$\gamma(h) = \frac{1}{2N(h)}\sum_{i=1}^{N(\mathbf{h})} \big[Z(x_i) - Z(x_i + h)\big]^2$$

Свойства вариограммы:

- Вариограмма симметрична:

  $$\gamma(x) = \gamma(-x)$$
  
- Вариограмма связана с дисперсией:

  $$\gamma(\infty) = \Var\big[ Z(x) \big]$$
  
- Вариограмма связана с ковариацией:

  $$\gamma(h) = \Var\big[ Z(x) \big] - C(h)$$
  
- Вариограмма чувствительна к аномальным значениям (по причине второй степени)
  
### Модели вариограмм

В уравнениях кригинга нельзя использовать эмпирическую вариограмму. Это связано с тем, что вывод этих уравнений опирается на предположение, что вариограмма представляет собой _условно положительно определенную функцию_. Поэтому следующим шагом после вычисления эмпирической вариограммы является подбор теоретической модели, которая наилучшим образом аппроксимирует эмпирическу функцию, и при этом отвечает требованию условной положительной определенности. Рассмотрим несколько распространенных на практике моделей.

### Сферическая модель

$$\gamma(h) = \begin{cases}
  c_0 + c\Big[\frac{3h}{2a} - \frac{1}{2}\big(\frac{h}{a}\big)^3\Big], & h \leq a; \\
  c_0 + c, & h > a.
\end{cases}$$

$$\gamma(a) = \Var\big[Z(p)\big] = c_0 + c$$

```{r, fig.height=3, dpi=300, fig.align='center'}
n = 60
a = 40
h = 0:n

tab = tibble::tibble(
  h = 0:60,
  gamma = c(3 * (0:(a-1)) / (2 * a) - 0.5 * (0:(a-1) / a)^3, rep(1, n-a+1))
)

ggplot() +
  geom_line(tab, mapping = aes(h, gamma), size = 1, color = 'steelblue') +
  geom_vline(xintercept = a, color = 'orangered') +
  annotate("text", x = a + 3, y = 0.5625, label = paste("a =", a), color = 'orangered') + 
  theme_bw()
```

Данная модель достигает плато в точке $h = a$.


### Экспоненциальная модель

$$\gamma(h) = \begin{cases}
  0, & h = 0; \\
  c_0 + (c-c_0)\Big[1 - \exp\big(\frac{-3h}{a}\big)\Big], & h \neq 0.
\end{cases}$$

$$\gamma(a) = \Var\big[Z(p)\big] = c_0 + c$$

```{r, fig.height=3, dpi=300, fig.align='center'}
tab = tibble::tibble(
  h = h,
  gamma = 1 - exp(-3*h/a)
)

pl = ggplot() +
  geom_line(tab, mapping = aes(h, gamma), size = 1, color = 'steelblue') +
  geom_vline(xintercept = a, color = 'orangered') +
  annotate("text", x = a + 3, y = 0.5625, label = paste("a =", a), color = 'orangered') + 
  theme_bw()

(pl)
```

- Данная модель достигает плато асимптотически.
- В точке $h = a$ достигается $95\%$ уровня плато.

### Гауссова модель

$$\gamma(h) = c_0 + c\Bigg[1 - \exp\bigg(\frac{-3h^2}{a^2}\bigg)\Bigg]$$

```{r, fig.height=3, dpi=300, fig.align='center'}
tab = tibble::tibble(
  h = h,
  gamma = 1 - exp(-3*h^2/a^2)
)

pl = ggplot() +
  geom_line(tab, mapping = aes(h, gamma), size = 1, color = 'steelblue') +
  geom_vline(xintercept = a, color = 'orangered') +
  annotate("text", x = a + 3, y = 0.5625, label = paste("a =", a), color = 'orangered') + 
  theme_bw()

(pl)
```

- Данная модель достигает плато асимптотически.
- В точке $h = a$ достигается $95\%$ уровня плато.
- Отличительной чертой этой модели является ее гладкость: параболическое поведение вблизи нуля и асимптотическое приближение к плато. 

### Степенная модель

$$\gamma(h) = \begin{cases}
  0, & h = 0; \\
  c h^\alpha, & h \neq 0.
\end{cases}$$

```{r, fig.height=3, dpi=300, fig.align='center'}
tab = tibble::tibble(
  h = h,
  gamma = h^1.5
)

pl = ggplot() +
  geom_line(tab, mapping = aes(h, gamma), size = 1, color = 'steelblue') +
  theme_bw()

(pl)
```

- Автокорреляция присутствует на всех расстояниях: $a \rightarrow \infty$
- Предположение о стационарности второго порядка не выполняется
- Как правило, это означает наличие тренда в данных

### Эффект самородка (модель наггет)

$$\gamma(h) = \begin{cases}
  0, & h = 0; \\
  c_0, & h \neq 0.
\end{cases}, ~ c_0 = C(0)$$

```{r sam, fig.height=3, fig.align='center', dpi=300}
tab = tibble::tibble(
  gamma = rep(1, n+1),
  h = h
)

ggplot() +
  geom_line(tab, mapping = aes(h, gamma), size = 1, color = 'steelblue') +
  geom_point(data = data.frame(x = 0, y = 1), mapping = aes(x, y), shape=21,
             colour = 'steelblue', fill = 'white', size = 3, stroke = 1.5) +
  annotate('point', x = 0, y = 0, color = 'steelblue', size = 4) +
  theme_bw()
```

- Наличие у данных вариограммы типа наггет означает отсутствие пространственной корреляции.
- Возможные причины:
  - Абсолютно случайное распределение
  - Мелкомасштабная вариабельность (меньше, чем расстояние между измерениями)
  - Ошибки в измерениях
  - Ошибки в координатах точек

### Диаграмма рассеяния с лагом

__Lagged scatterplot__ — вариант диаграммы рассеяния, на котором показываются значения в точках, расстояние между которыми попадает в заданный интервал 

```{r, fig.height=3.5, dpi=300, fig.align='center'}
options(scipen = 999)

cities = st_read("data/Italy_Cities.gpkg")

rainfall = read_table2("data/Rainfall.dat") %>% 
  st_as_sf(coords = c('x', 'y'), 
           crs = st_crs(cities),
           remove = FALSE)

hscat(rain_24~1, data = rainfall, 1000 * c(0, 10, 20, 50, 100), pch = 19)
```

### Вариограммное облако

Квадрат разности значений как функция от расстояния между точками
```{r, fig.height=4, dpi=300}
varcl = variogram(rain_24~1, data=rainfall, cutoff = 150000, cloud=TRUE)

ggplot(varcl) +
  geom_point(aes(dist, gamma), alpha = 0.5, size = 2, color = 'steelblue') +
  ylab('semivariance') +
  theme_bw()
```
  

### Эмпирическая вариограмма

Эмпирическая вариограмма рассчитывается путем разбения вариограммного облака на интервалы расстояний — __лаги__ — и подсчета среднего значения $\gamma$ в каждом лаге по следующей формуле:

$$\hat{\gamma} = \frac{1}{2N_h} \sum_{x_i - x_j \approx h} \big[z(x_i) - z(x_j)\big]^2$$

```{r, fig.height=3.5, dpi=300}
width = 10000
intervals = width * 0:15

vargr = variogram(rain_24~1, data=rainfall, cutoff = 150000, width = width)
```

```{r, echo = FALSE}
ggplot() +
  geom_bin2d(varcl, mapping = aes(dist, gamma), binwidth = c(width, 3000)) +
  geom_point(varcl, mapping = aes(dist, gamma), alpha = 0.5, size = 2, color = 'steelblue') +
  geom_vline(xintercept = intervals, size = 0.25) +
  geom_vline(xintercept = 1e4 * 6:7, size = 1, color = 'white') +
  geom_label(aes(x = 30000, y = 2700, label = "лаг [60 000, 70 000)"), label.padding = unit(0.35, "lines"),
             color = 'forestgreen', fill = 'white', size = 4.5, alpha = 0.8) + 
  geom_line(vargr, mapping = aes(dist, gamma)) +
  geom_point(vargr, mapping = aes(dist, gamma, size = np)) +
  scale_fill_continuous(low = 'bisque', high = 'coral3') +
  scale_size(range = c(1, 3)) +
  coord_cartesian(ylim = c(0, 3000)) +
  scale_x_continuous(expand = c(0, 0)) + 
  scale_y_continuous(expand = c(0, 0)) +
  theme_bw()
```

Оставив только вариограмму, получим:
```{r, fig.height=3, dpi=300}
ggplot() +
  geom_line(vargr, mapping = aes(dist, gamma)) +
  geom_point(vargr, mapping = aes(dist, gamma, size = np)) +
  scale_size(range = c(1, 5)) +
  theme_bw()
```
Размер точки означает количество пар значений, которые попали в каждый лаг.

Поскольку вариограмма есть _дисперсия разности значений_, ее рост при увеличении расстояния можно оценить также по увеличению размера «ящика» на диаграмме размаха $\sqrt\gamma$:

```{r, fig.height=3.5, dpi=300}
varcl = varcl %>% 
  mutate(sqgamma = sqrt(gamma),
         lag = cut(dist, breaks = intervals, labels = 0.001 * (intervals[-1] - 0.5*width)))

ggplot(varcl) +
  geom_boxplot(aes(lag, sqrt(gamma)), outlier.alpha = 0.1)
```

### Вариокарта

__Вариокарта__ (_variogram map, variomap_) представляет вариограмму как функцию приращений координат:
$$\hat{\gamma} (\Delta x, \Delta y) = \frac{1}{2N_{\substack{\Delta x\\ \Delta y}}} \sum_{\substack{\Delta x_{ij} \approx \Delta x\\ \Delta y_{ij} \approx \Delta y}} \big[z(p_i) - z(p_j)\big]^2$$

```{r, fig.height=5, dpi=300}
varmp = variogram(rain_24~1, data=rainfall, cutoff = 150000, width = width, map = TRUE)[['map']]
```

```{r, echo = FALSE}
dims = unname(varmp@grid@cells.dim)
offs = varmp@grid@cellcentre.offset

m = varmp@data %>% pull(var1) %>% matrix(dims)
dim(m) = c(dx = dims[1], dy = dims[2])

varstars = st_as_stars(gamma = m)

attr(varstars, "dimensions")[[1]]$offset = offs[1]
attr(varstars, "dimensions")[[2]]$offset = -offs[2]

attr(varstars, "dimensions")[[1]]$delta = width
attr(varstars, "dimensions")[[2]]$delta = -width

ggplot() +
  geom_stars(data = varstars) +
  scale_fill_continuous(type = "viridis") +
  coord_equal() +
  theme_bw() +
  theme(axis.text = element_text(size=18),
        legend.text =  element_text(size=14),
        legend.title =  element_text(size=18),
        axis.title = element_text(size=18,face="bold"))
```


Вариокарта используется для выявления _пространственной анизотропии_. Профиль по линии из центра к краю вариокарты даст эмпирическую вариограмму

### Приближение теоретической модели

__Приближение__ (_fitting_) модели вариограммы предполагает:

1. Выбор теоретической модели
2. Подбор параметров модели: эффект самородка (nugget), радиус корреляции и плато.

```{r, echo = FALSE, fig.height=3, dpi=300}
nugget = 15
sill = 215
lag = 1000
a = 120000
cutoff = 150000

h0 = lag * 0:(a/lag)
h1 = lag * (a/lag + 1):(cutoff/lag) 

tab1 = tibble::tibble(
  h = c(h0, h1),
  gamma = c(nugget + (sill - nugget) * (3 * h0 / (2 * a) - 0.5 * (h0 / a)^3), rep(sill, length(h1))),
  fit = 'manual'
)

vargr = variogram(rain_24~1, data=rainfall, cutoff = 150000, width = width)

ggplot() +
  geom_vline(xintercept = a, color = 'orangered') +
  annotate("text", x = a + 1.5 * width, y = 0.5 * sill, label = paste("a =", a), color = 'orangered') + 
  geom_hline(yintercept = sill, color = 'orangered') +
  annotate("text", x = 0.5 * a, y = sill * 1.05, label = paste("sill =", sill), color = 'orangered') + 
  geom_line(vargr, mapping = aes(dist, gamma)) +
  geom_point(vargr, mapping = aes(dist, gamma), size = 2) +
  scale_size(range = c(1, 5)) +
  geom_line(tab1, mapping = aes(h, gamma), size = 1, color = 'steelblue') +
  geom_point(data.frame(h = 0, gamma = nugget), mapping = aes(h, gamma), size = 3, color = 'orangered') +
  annotate("text", x = 1.5 * width, y = nugget, label = paste("nugget =", nugget), color = 'orangered') + 
  xlab('lag') + ylab('gamma') +
  ggtitle('Сферическая модель') +
  theme_bw()
```

Дана вариограмма семейства $\gamma (h; \mathbf{b})$, где $\mathbf{b} = (b_1, ..., b_k)$ — вектор из $k$ параметров модели. Параметры $\mathbf{b}$ подбираются таким образом, чтобы минимизировать следующий функционал:

$$Q(\mathbf{b}) = \sum_{l=1}^{L} w_l \big[\hat{\gamma}(h_l) - \gamma (h; \mathbf{b})\big]^2,$$

где $\big\{\hat{\gamma} (h_l): l = 1,...,L\big\}$ — значения эмпирической вариограммы для $L$ лагов, вычисленные по $N(h_l)$ векторам.

Веса $w_l$ обычно выбираются исходя из отношения $w_l = N(h_l) / |h_l|$, чтобы придать большее значение коротким расстояниям и лагам с хорошей оценкой.

Минимизация функционала осуществляется итеративно:

1. Процесс начинается с некоторого предположения $\mathbf{b}^{(0)}$
2. На шаге $s$ функция $Q$ аппроксимируется в виде квадратичной формы $Q(\mathbf{b}^{(s)}) \approx \sum_{i=1}^k \sum_{j=1}^k \delta_{ij} b_i b_j$ путем разложения в ряд Тейлора вокруг точки $\mathbf{b}^{(s)}$.
3. Новая точка минимума $\mathbf{b}^{(s+1)}$ находится как минимум квадратичной формы (_этот минимум один_).

Шаги 2-3 повторяются до тех пор, пока значение $Q$ не станет меньше заданного порога.

Сравним результат ручного и автоматического приближения вариограммы:

```{r, fig.height=4, dpi=300}
varmd = fit.variogram(vargr, model = vgm(psill = 215, model = 'Sph', range = 120000, nugget = 15))

h0 = lag * 0:(varmd[2, 'range']/lag)
h1 = lag * (varmd[2, 'range']/lag + 1):(cutoff/lag) 

tab2 = tibble::tibble(
  h = c(h0, h1),
  gamma = c(varmd[1, 'psill'] + (varmd[2, 'psill'] * (3 * h0 / (2 * varmd[2, 'range']) - 0.5 * (h0 / varmd[2, 'range'])^3)), rep(varmd[1, 'psill'] + varmd[2, 'psill'], length(h1))),
  fit = 'automatic'
)

tab = bind_rows(tab1, tab2)

ggplot() +
  geom_line(vargr, mapping = aes(dist, gamma)) +
  geom_point(vargr, mapping = aes(dist, gamma), size = 2) +
  scale_size(range = c(1, 5)) +
  geom_line(tab, mapping = aes(h, gamma, color = fit), size = 1) +
  xlab('lag') + ylab('gamma') +
  ggtitle('Сферическая модель') +
  theme_bw()
```

### Обычный кригинг

Рассмотрим данные по температуре:
```{r, warning=FALSE, dpi = 300, fig.height = 4}
box = st_bbox(rainfall)
envelope = box[c(1,3,2,4)]

px_grid = st_as_stars(box, dx = 2000, dy = 2000)

ggplot() + 
  geom_sf(data = rainfall, color = 'red') +
  geom_sf(data = st_as_sf(px_grid), size = 0.5, fill = NA)
```

### Обычный кригинг

Визуализируем найденную вариограмму и вариокарту:
```{r}
plot(vargr, model = varmd)
plot(varmp)
```

Проинтерполируем, используя приближенную модель вариограммы:
```{r}
(px_grid = krige(rain_24~1, rainfall, px_grid, model = varmd))
```

### Оценка и дисперсия кригинга

```{r, warning=FALSE, dpi = 300, fig.height=4}
rain_colors = colorRampPalette(c("white", "dodgerblue", "dodgerblue4"))
rain_levels = seq(0,80,by=10)
rain_ncolors = length(rain_levels)-1

err_colors = colorRampPalette(c("white", "coral", "violetred"))
err_levels = seq(0, 180, by = 20)
err_ncolors = length(err_levels) - 1

cont = st_contour(px_grid['var1.pred'], breaks = rain_levels, contour_lines = TRUE)
conterr = st_contour(px_grid['var1.var'], breaks = err_levels, contour_lines = TRUE)

ggplot() +
  geom_stars(data = cut(px_grid['var1.pred'], breaks = rain_levels)) +
  scale_fill_manual(name = 'мм',
                    values = rain_colors(rain_ncolors),
                    labels = paste(rain_levels[-rain_ncolors-1], '-', rain_levels[-1]),
                    drop = FALSE) +
  coord_sf(crs = st_crs(rainfall)) +
  geom_sf(data = cont, color = 'black', size = 0.2) +
  geom_sf(data = rainfall, color = 'black', size = 0.3) +
  theme_bw()

ggplot() +
  geom_stars(data = cut(px_grid['var1.var'], breaks = err_levels)) +
  scale_fill_manual(name = 'мм',
                    values = err_colors(err_ncolors),
                    labels = paste(err_levels[-err_ncolors-1], '-', err_levels[-1]),
                    drop = FALSE) +
  coord_sf(crs = st_crs(rainfall)) +
  geom_sf(data = conterr, color = 'black', size = 0.2) +
  geom_sf(data = rainfall, color = 'black', size = 0.3) +
  theme_bw()
```

Дисперсия кригинга высока там, где мало данных.

### Кросс-валидация

Для выполнения кросс-валидации воспользуемся функцией `krige.cv`:
```{r, warning=FALSE, dpi = 300, fig.height=4}
cvl = krige.cv(rain_24~1, rainfall, varmd) %>% 
  st_as_sf() %>% 
  mutate(sterr = residual / sqrt(var1.var))

head(cvl %>% st_set_geometry(NULL), 10)
```

Cтандартизированные ошибки в стационарном случае должны быть распределены нормально:

```{r, message=FALSE, warning=FALSE, dpi = 300, fig.height=4}
ggplot(cvl, aes(x = sterr)) +
  geom_histogram(aes(y = stat(density)), fill = 'grey', color = 'black', size = 0.1) +
  geom_density(fill = 'olivedrab', alpha = 0.5) +
  theme_bw()
```

Ошибки должны быть независимы от значений:

```{r, message=FALSE, warning=FALSE, dpi = 300, fig.height=4}
ggplot(cvl, aes(x = var1.pred, sterr)) +
  geom_point(alpha = 0.8) +
  geom_smooth(method = 'lm') +
  theme_bw()

cor.test(~ sterr + var1.pred, data = cvl)
```

Облако рассеяния оценки относительно истинных значений должно быть компактным:

```{r, message=FALSE, warning=FALSE, dpi = 300, fig.height=4}
ggplot(cvl, aes(x = var1.pred, observed)) +
  geom_point(alpha = 0.8) +
  geom_smooth(method = 'lm') +
  theme_bw()

# Диагностика модели линейной регрессии
summary(lm(observed ~ var1.pred, cvl))
```

Пространственная картина стандартизированных ошибок должна быть гомогенной:
```{r, warning=FALSE, dpi = 300, fig.height=3, fig.width=7}
library(akima)

coords = st_coordinates(rainfall)
coords_grid = st_coordinates(px_grid)

px_grid = px_grid %>% 
  mutate(sterr = interpp(x = coords[,1],
                         y = coords[,2],
                         z = cvl$sterr, 
                         xo = coords_grid[,1],
                         yo = coords_grid[,2],
                         linear = FALSE,
                         extrap = TRUE)$z)

sterr_levels = seq(-8,8,2)
sterr_ncolors = length(sterr_levels)-1
sterr_colors = colorRampPalette(c('blue', 'white', 'red'))

sterrcont = st_contour(px_grid['sterr'], breaks = sterr_levels, contour_lines = TRUE)

ggplot() +
  geom_stars(data = cut(px_grid['sterr'], breaks = sterr_levels)) +
  scale_fill_manual(name = 'мм',
                    values = sterr_colors(sterr_ncolors),
                    labels = paste(sterr_levels[-sterr_ncolors-1], '-', sterr_levels[-1]),
                    drop = FALSE) +
  coord_sf(crs = st_crs(rainfall)) +
  geom_sf(data = sterrcont, color = 'black', size = 0.2) +
  geom_sf(data = rainfall, color = 'black', size = 0.3) +
  theme_bw()
```

## Краткий обзор {#geostat_review}

Для просмотра презентации щелкните на ней один раз левой кнопкой мыши и листайте, используя кнопки на клавиатуре:
```{r, echo=FALSE}
knitr::include_url('https://tsamsonov.github.io/r-geo-course/slides/14-Geostatistics_slides.html#1', height = '500px')
```

> Презентацию можно открыть в отдельном окне или вкладке браузере. Для этого щелкните по ней правой кнопкой мыши и выберите соответствующую команду.

## Контрольные вопросы и упражнения {#geostat_qt}

### Вопросы {#geostat_q}

1. Дайте определения случайной величины, случайного процесса, сечения случайного процесса, реализации случайного процесса. Проиллюстрируйте их примерами.
1. Сформулируйте определение основных моментов пространственного случайного процесса: математического ожидания, дисперсии и ковариации. Поясните суть этих понятий на конкретных примерах.
1. Дайте определение стационарности в строгом смысле слова и стационарности второго порядка.
1. При каком условии случайная функция является изотропной?
1. Что такое эргодичность? Пояснить суть этого свойства на наглядном примере.
1. Сформулируйте суть кригинга как метода интерполяции данных. В чем его сходства и отличия в сравнении с методом радиальных базисных функций?
1. Что из себя представляет среднеквадратическая ошибка, минимизируемая в методе кригинга?
1. Почему среднеквадратическую ошибку можно считать равной дисперсии в методе простого кригинга?
1. Что характеризует дисперсия кригинга?
1. Напишите систему уравнений простого кригинга и выражение для дисперсии простого кригинга.
1. Дайте определение стационарности приращений. С какой целью вводится данный тип стационарных процессов?
1. Какие условия должны быть выполнены для того чтобы ковариацию можно было заменить вариограммой в уравнениях кригинга?
1. Напишите систему уравнений обычного кригинга и выражение для дисперсии обычного кригинга. Чем эти уравнения отличаются от уравнений простого кригинга?
1. Изложите модель универсального кригинга в математической и словесной форме.
1. Что такое базисные функции и ковариаты? Какую роль они выполняют в методе универсального кригинга?
1. Сформулируйте условия универсальности, от которых ведет свое название метод универсального кригинга.
1. Напишите систему уравнений универсального кригинга и выражение для дисперсии универсального кригинга.
1. Перечислите стандартный набор действий, применяемых в рамках выполнения процедуры кросс-валидации результатов кригинга.
1. Что такое вариограмма? Дайте математическое и словесное определение.
1. Перечислите свойства вариограммы.
1. Назовите основные модели вариограммы. Какая модель свидетельствует о наличии пространственного тренда в данных?
1. Чем эмпирическая вариаограмма отличается от теоретической (модели)?
1. Сформулируйте принципы построения и назначение основных диагностических графиков вариографии: диаграммы рассеяния с лагом, вариограммного облака, эмпирической вариограммы, вариокарты.
1. Объясните, каким образом можно получить приближение (подгонку) теоретической модели вариограммы под эмпирические данные.
1. Расскажите об основных возможностях пакета __gstat__. Перечислите функции этого пакета, которые используются для вариографии и интерполяции методом кригинга.

### Упражнения {#geostat_t}

1. Загрузите [данные](https://raw.githubusercontent.com/tsamsonov/r-geo-course/master/data/argo.csv) дрейфующих буев [__ARGO__](http://www.argo.ucsd.edu/About_Argo.html) на акваторию Северной Атлантики за 30 января 2010 года. Постройте поля распределения солености и температуры методом обычного кригинга с размером ячейки 50 км. Подберите подходящую модель вариограммы. Выполните визуализацию оценки кригинга и дисперсии кригинга средствами __ggplot2__. Произведите кросс-валидацию полученных результатов.

    > __Подсказка:__ перед построением сетки интерполяции странсформируйте данные в проекцию [__Меркатора__](http://spatialreference.org/ref/epsg/wgs-84-world-mercator/). Чтобы полученное поле распределения покрывало только акваторию, маскируйте полученный растр с использованием слоя _ocean_ из набора данных __Natural Earth__. Перед выполнением маскирования преобразуйте мультиполигон в обычные полигоны (в противном случае маскирование отработает некорректно).

----
_Самсонов Т.Е._ **Визуализация и анализ географических данных на языке R.** М.: Географический факультет МГУ, `r lubridate::year(Sys.Date())`. DOI: [10.5281/zenodo.901911](https://doi.org/10.5281/zenodo.901911)
----